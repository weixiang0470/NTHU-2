{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be663292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mypdata import get_train_xy,resample_xy,get_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e8c85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    93028\n",
      "1    93028\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X,y = get_train_xy(\"train.csv\")\n",
    "X_train, X_test, y_train, y_test=get_split(X,y)\n",
    "X_resampled, y_resampled = resample_xy(X,y)\n",
    "Xr_train, Xr_test, yr_train, yr_test=get_split(X_resampled,y_resampled)\n",
    "print(y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c984d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    93028\n",
      "1    21486\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def get_models(scale_pos_weight):\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=201,\n",
    "        max_depth=7,\n",
    "        class_weight={0: 1, 1: scale_pos_weight}, \n",
    "        max_samples=0.8,     \n",
    "        max_features=0.7,     \n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=201,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        gamma=0.1,\n",
    "        tree_method='hist',\n",
    "        # use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    lgb_model = LGBMClassifier(\n",
    "        n_estimators=201,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        class_weight={0: 1, 1: scale_pos_weight}, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    meta_model = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.05,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.1,\n",
    "        tree_method='hist',\n",
    "        # use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    base_rf = RandomForestClassifier(\n",
    "        n_estimators=200,       \n",
    "        max_depth=5,            \n",
    "        min_samples_split=5,      \n",
    "        min_samples_leaf=2,        \n",
    "        max_samples=0.5,   \n",
    "        max_features=0.65,\n",
    "        bootstrap=True,            \n",
    "        class_weight='balanced',    \n",
    "        random_state=42,            \n",
    "    )\n",
    "\n",
    "    ada_model = AdaBoostClassifier(\n",
    "        estimator=base_rf, \n",
    "        n_estimators=100,        \n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return rf_model,xgb_model,lgb_model,meta_model,ada_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17273, number of negative: 74338\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 91611, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501505 -> initscore=0.006021\n",
      "[LightGBM] [Info] Start training from score 0.006021\n",
      "[LightGBM] [Info] Number of positive: 13818, number of negative: 59470\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 73288, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501500 -> initscore=0.005999\n",
      "[LightGBM] [Info] Start training from score 0.005999\n",
      "[LightGBM] [Info] Number of positive: 13819, number of negative: 59470\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 73289, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501518 -> initscore=0.006071\n",
      "[LightGBM] [Info] Start training from score 0.006071\n",
      "[LightGBM] [Info] Number of positive: 13819, number of negative: 59470\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 73289, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501518 -> initscore=0.006071\n",
      "[LightGBM] [Info] Start training from score 0.006071\n",
      "[LightGBM] [Info] Number of positive: 13818, number of negative: 59471\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 73289, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501496 -> initscore=0.005982\n",
      "[LightGBM] [Info] Start training from score 0.005982\n",
      "[LightGBM] [Info] Number of positive: 13818, number of negative: 59471\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 73289, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501496 -> initscore=0.005982\n",
      "[LightGBM] [Info] Start training from score 0.005982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.70      0.79     18690\n",
      "           1       0.34      0.69      0.46      4213\n",
      "\n",
      "    accuracy                           0.70     22903\n",
      "   macro avg       0.63      0.70      0.62     22903\n",
      "weighted avg       0.81      0.70      0.73     22903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "scale_pos_weight = (len(y) - sum(y)) / sum(y)\n",
    "\n",
    "rf_model,xgb_model,lgb_model,meta_model,ada_model = get_models(scale_pos_weight)\n",
    "\n",
    "stacked_boost = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "stacked_boost.fit(X_train, y_train)\n",
    "\n",
    "y_pred = stacked_boost.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c68c4411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 21486, number of negative: 93028\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 114514, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 17189, number of negative: 74422\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 91611, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000017\n",
      "[LightGBM] [Info] Start training from score 0.000017\n",
      "[LightGBM] [Info] Number of positive: 17189, number of negative: 74422\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 91611, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000017\n",
      "[LightGBM] [Info] Start training from score 0.000017\n",
      "[LightGBM] [Info] Number of positive: 17189, number of negative: 74422\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 91611, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000017\n",
      "[LightGBM] [Info] Start training from score 0.000017\n",
      "[LightGBM] [Info] Number of positive: 17188, number of negative: 74423\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 91611, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499986 -> initscore=-0.000055\n",
      "[LightGBM] [Info] Start training from score -0.000055\n",
      "[LightGBM] [Info] Number of positive: 17189, number of negative: 74423\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8540\n",
      "[LightGBM] [Info] Number of data points in the train set: 91612, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500001 -> initscore=0.000004\n",
      "[LightGBM] [Info] Start training from score 0.000004\n"
     ]
    }
   ],
   "source": [
    "rf_model,xgb_model,lgb_model,meta_model,ada_model = get_models(scale_pos_weight)\n",
    "\n",
    "stacked_boost = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True\n",
    ")\n",
    "model1 = stacked_boost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cccdb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "074a7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = model1.predict(df_test.drop(\"index\",axis=1))\n",
    "# y2_pred = model2.predict(df_test.drop(\"index\",axis=1))\n",
    "\n",
    "results1 = pd.DataFrame({\n",
    "    \"index\": df_test[\"index\"],  # 保留原始 ID\n",
    "    \"target\": y1_pred\n",
    "})\n",
    "\n",
    "# results2 = pd.DataFrame({\n",
    "#     \"index\": df_test[\"index\"],  # 保留原始 ID\n",
    "#     \"target\": y2_pred\n",
    "# })\n",
    "\n",
    "results1.to_csv(\"predictions1.csv\", index=False)\n",
    "# results2.to_csv(\"predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d8e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nthu_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
