{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9d8d2c-d0c8-4913-9857-2c0738e2ccb8",
   "metadata": {},
   "source": [
    "# HW 2-2 Add more statistics to analyze the an ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ca359-c4e3-4269-bff2-2a9c6721923f",
   "metadata": {},
   "source": [
    "## 2-2-1. model characteristics\n",
    "\n",
    "- operator types\n",
    "    - Print the number of onnx operators, list out unique onnx operator names\n",
    "- For each operator\n",
    "    - print out the attributes ranges\n",
    "    - e.g. For a Conv2D layer, print its width, height, channel, dialation, stride, and kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "58bb3546-fa6d-43b0-967a-d3e94a7470ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv', 'Clip', 'Add', 'GlobalAveragePool', 'Shape', 'Constant', 'Gather', 'Unsqueeze', 'Concat', 'Reshape', 'Gemm']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load('./mobilenetv2-10.onnx')\n",
    "\n",
    "# The model is represented as a protobuf structure and it can be accessed\n",
    "# using the standard python-for-protobuf methods\n",
    "\n",
    "## list all the operator types in the model\n",
    "node_list = []\n",
    "# count = []\n",
    "for i in onnx_model.graph.node:\n",
    "    if (i.op_type not in node_list):\n",
    "        node_list.append(i.op_type)\n",
    "        # count.append(1)\n",
    "    else:\n",
    "        idx = node_list.index(i.op_type)\n",
    "        # count[idx] = count[idx]+1\n",
    "print(node_list)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b00409e-c814-46ef-9205-004d86513edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(shape):\n",
    "    dims = []\n",
    "    size = 1\n",
    "    # print((shape))\n",
    "    for dim in shape.dim:\n",
    "        if dim.dim_value > 0:\n",
    "            dims.append(dim.dim_value)\n",
    "            size *= dim.dim_value\n",
    "        elif dim.dim_param:\n",
    "            dims.append(1)  # 保存符號名稱\n",
    "            size = 1  # 表示無法計算完整大小\n",
    "        else:\n",
    "            dims.append(-1)  # 未定義維度\n",
    "            size = 0\n",
    "    return dims, size\n",
    "def get_size2(shape):\n",
    "    dims = shape.dims\n",
    "    size = 1\n",
    "    for dim in dims:\n",
    "        size *= dim\n",
    "    return dims, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0f8b8221-dc12-42c9-b21b-09363125ade2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Operator 'Conv' (name: 'Conv_0') --\n",
      "input : [1, 3, 224, 224]\n",
      "475 : [32, 3, 3, 3]\n",
      "476 : [32]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [2, 2]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_1') --\n",
      "474 : [1, 32, 112, 112]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_2') --\n",
      "317 : [1, 32, 112, 112]\n",
      "478 : [32, 1, 3, 3]\n",
      "479 : [32]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 32\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_3') --\n",
      "477 : [1, 32, 112, 112]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_4') --\n",
      "320 : [1, 32, 112, 112]\n",
      "481 : [16, 32, 1, 1]\n",
      "482 : [16]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_5') --\n",
      "480 : [1, 16, 112, 112]\n",
      "484 : [96, 16, 1, 1]\n",
      "485 : [96]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_6') --\n",
      "483 : [1, 96, 112, 112]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_7') --\n",
      "325 : [1, 96, 112, 112]\n",
      "487 : [96, 1, 3, 3]\n",
      "488 : [96]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 96\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [2, 2]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_8') --\n",
      "486 : [1, 96, 56, 56]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_9') --\n",
      "328 : [1, 96, 56, 56]\n",
      "490 : [24, 96, 1, 1]\n",
      "491 : [24]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_10') --\n",
      "489 : [1, 24, 56, 56]\n",
      "493 : [144, 24, 1, 1]\n",
      "494 : [144]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_11') --\n",
      "492 : [1, 144, 56, 56]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_12') --\n",
      "333 : [1, 144, 56, 56]\n",
      "496 : [144, 1, 3, 3]\n",
      "497 : [144]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 144\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_13') --\n",
      "495 : [1, 144, 56, 56]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_14') --\n",
      "336 : [1, 144, 56, 56]\n",
      "499 : [24, 144, 1, 1]\n",
      "500 : [24]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_15') --\n",
      "489 : [1, 24, 56, 56]\n",
      "498 : [1, 24, 56, 56]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_16') --\n",
      "339 : [1, 24, 56, 56]\n",
      "502 : [144, 24, 1, 1]\n",
      "503 : [144]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_17') --\n",
      "501 : [1, 144, 56, 56]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_18') --\n",
      "342 : [1, 144, 56, 56]\n",
      "505 : [144, 1, 3, 3]\n",
      "506 : [144]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 144\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [2, 2]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_19') --\n",
      "504 : [1, 144, 28, 28]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_20') --\n",
      "345 : [1, 144, 28, 28]\n",
      "508 : [32, 144, 1, 1]\n",
      "509 : [32]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_21') --\n",
      "507 : [1, 32, 28, 28]\n",
      "511 : [192, 32, 1, 1]\n",
      "512 : [192]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_22') --\n",
      "510 : [1, 192, 28, 28]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_23') --\n",
      "350 : [1, 192, 28, 28]\n",
      "514 : [192, 1, 3, 3]\n",
      "515 : [192]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 192\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_24') --\n",
      "513 : [1, 192, 28, 28]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_25') --\n",
      "353 : [1, 192, 28, 28]\n",
      "517 : [32, 192, 1, 1]\n",
      "518 : [32]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_26') --\n",
      "507 : [1, 32, 28, 28]\n",
      "516 : [1, 32, 28, 28]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_27') --\n",
      "356 : [1, 32, 28, 28]\n",
      "520 : [192, 32, 1, 1]\n",
      "521 : [192]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_28') --\n",
      "519 : [1, 192, 28, 28]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_29') --\n",
      "359 : [1, 192, 28, 28]\n",
      "523 : [192, 1, 3, 3]\n",
      "524 : [192]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 192\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_30') --\n",
      "522 : [1, 192, 28, 28]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_31') --\n",
      "362 : [1, 192, 28, 28]\n",
      "526 : [32, 192, 1, 1]\n",
      "527 : [32]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_32') --\n",
      "356 : [1, 32, 28, 28]\n",
      "525 : [1, 32, 28, 28]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_33') --\n",
      "365 : [1, 32, 28, 28]\n",
      "529 : [192, 32, 1, 1]\n",
      "530 : [192]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_34') --\n",
      "528 : [1, 192, 28, 28]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_35') --\n",
      "368 : [1, 192, 28, 28]\n",
      "532 : [192, 1, 3, 3]\n",
      "533 : [192]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 192\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [2, 2]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_36') --\n",
      "531 : [1, 192, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_37') --\n",
      "371 : [1, 192, 14, 14]\n",
      "535 : [64, 192, 1, 1]\n",
      "536 : [64]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_38') --\n",
      "534 : [1, 64, 14, 14]\n",
      "538 : [384, 64, 1, 1]\n",
      "539 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_39') --\n",
      "537 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_40') --\n",
      "376 : [1, 384, 14, 14]\n",
      "541 : [384, 1, 3, 3]\n",
      "542 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 384\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_41') --\n",
      "540 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_42') --\n",
      "379 : [1, 384, 14, 14]\n",
      "544 : [64, 384, 1, 1]\n",
      "545 : [64]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_43') --\n",
      "534 : [1, 64, 14, 14]\n",
      "543 : [1, 64, 14, 14]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_44') --\n",
      "382 : [1, 64, 14, 14]\n",
      "547 : [384, 64, 1, 1]\n",
      "548 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_45') --\n",
      "546 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_46') --\n",
      "385 : [1, 384, 14, 14]\n",
      "550 : [384, 1, 3, 3]\n",
      "551 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 384\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_47') --\n",
      "549 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_48') --\n",
      "388 : [1, 384, 14, 14]\n",
      "553 : [64, 384, 1, 1]\n",
      "554 : [64]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_49') --\n",
      "382 : [1, 64, 14, 14]\n",
      "552 : [1, 64, 14, 14]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_50') --\n",
      "391 : [1, 64, 14, 14]\n",
      "556 : [384, 64, 1, 1]\n",
      "557 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_51') --\n",
      "555 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_52') --\n",
      "394 : [1, 384, 14, 14]\n",
      "559 : [384, 1, 3, 3]\n",
      "560 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 384\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_53') --\n",
      "558 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_54') --\n",
      "397 : [1, 384, 14, 14]\n",
      "562 : [64, 384, 1, 1]\n",
      "563 : [64]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_55') --\n",
      "391 : [1, 64, 14, 14]\n",
      "561 : [1, 64, 14, 14]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_56') --\n",
      "400 : [1, 64, 14, 14]\n",
      "565 : [384, 64, 1, 1]\n",
      "566 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_57') --\n",
      "564 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_58') --\n",
      "403 : [1, 384, 14, 14]\n",
      "568 : [384, 1, 3, 3]\n",
      "569 : [384]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 384\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_59') --\n",
      "567 : [1, 384, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_60') --\n",
      "406 : [1, 384, 14, 14]\n",
      "571 : [96, 384, 1, 1]\n",
      "572 : [96]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_61') --\n",
      "570 : [1, 96, 14, 14]\n",
      "574 : [576, 96, 1, 1]\n",
      "575 : [576]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_62') --\n",
      "573 : [1, 576, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_63') --\n",
      "411 : [1, 576, 14, 14]\n",
      "577 : [576, 1, 3, 3]\n",
      "578 : [576]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 576\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_64') --\n",
      "576 : [1, 576, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_65') --\n",
      "414 : [1, 576, 14, 14]\n",
      "580 : [96, 576, 1, 1]\n",
      "581 : [96]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_66') --\n",
      "570 : [1, 96, 14, 14]\n",
      "579 : [1, 96, 14, 14]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_67') --\n",
      "417 : [1, 96, 14, 14]\n",
      "583 : [576, 96, 1, 1]\n",
      "584 : [576]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_68') --\n",
      "582 : [1, 576, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_69') --\n",
      "420 : [1, 576, 14, 14]\n",
      "586 : [576, 1, 3, 3]\n",
      "587 : [576]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 576\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_70') --\n",
      "585 : [1, 576, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_71') --\n",
      "423 : [1, 576, 14, 14]\n",
      "589 : [96, 576, 1, 1]\n",
      "590 : [96]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_72') --\n",
      "417 : [1, 96, 14, 14]\n",
      "588 : [1, 96, 14, 14]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_73') --\n",
      "426 : [1, 96, 14, 14]\n",
      "592 : [576, 96, 1, 1]\n",
      "593 : [576]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_74') --\n",
      "591 : [1, 576, 14, 14]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_75') --\n",
      "429 : [1, 576, 14, 14]\n",
      "595 : [576, 1, 3, 3]\n",
      "596 : [576]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 576\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [2, 2]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_76') --\n",
      "594 : [1, 576, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_77') --\n",
      "432 : [1, 576, 7, 7]\n",
      "598 : [160, 576, 1, 1]\n",
      "599 : [160]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_78') --\n",
      "597 : [1, 160, 7, 7]\n",
      "601 : [960, 160, 1, 1]\n",
      "602 : [960]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_79') --\n",
      "600 : [1, 960, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_80') --\n",
      "437 : [1, 960, 7, 7]\n",
      "604 : [960, 1, 3, 3]\n",
      "605 : [960]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 960\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_81') --\n",
      "603 : [1, 960, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_82') --\n",
      "440 : [1, 960, 7, 7]\n",
      "607 : [160, 960, 1, 1]\n",
      "608 : [160]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_83') --\n",
      "597 : [1, 160, 7, 7]\n",
      "606 : [1, 160, 7, 7]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_84') --\n",
      "443 : [1, 160, 7, 7]\n",
      "610 : [960, 160, 1, 1]\n",
      "611 : [960]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_85') --\n",
      "609 : [1, 960, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_86') --\n",
      "446 : [1, 960, 7, 7]\n",
      "613 : [960, 1, 3, 3]\n",
      "614 : [960]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 960\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_87') --\n",
      "612 : [1, 960, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_88') --\n",
      "449 : [1, 960, 7, 7]\n",
      "616 : [160, 960, 1, 1]\n",
      "617 : [160]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Add' (name: 'Add_89') --\n",
      "443 : [1, 160, 7, 7]\n",
      "615 : [1, 160, 7, 7]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_90') --\n",
      "452 : [1, 160, 7, 7]\n",
      "619 : [960, 160, 1, 1]\n",
      "620 : [960]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_91') --\n",
      "618 : [1, 960, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_92') --\n",
      "455 : [1, 960, 7, 7]\n",
      "622 : [960, 1, 3, 3]\n",
      "623 : [960]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 960\n",
      "  kernel_shape: [3, 3]\n",
      "  pads: [1, 1, 1, 1]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_93') --\n",
      "621 : [1, 960, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_94') --\n",
      "458 : [1, 960, 7, 7]\n",
      "625 : [320, 960, 1, 1]\n",
      "626 : [320]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Conv' (name: 'Conv_95') --\n",
      "624 : [1, 320, 7, 7]\n",
      "628 : [1280, 320, 1, 1]\n",
      "629 : [1280]\n",
      "Attributes:\n",
      "  dilations: [1, 1]\n",
      "  group: 1\n",
      "  kernel_shape: [1, 1]\n",
      "  pads: [0, 0, 0, 0]\n",
      "  strides: [1, 1]\n",
      "\n",
      "-- Operator 'Clip' (name: 'Clip_96') --\n",
      "627 : [1, 1280, 7, 7]\n",
      "Attributes:\n",
      "  max: 6.0\n",
      "  min: 0.0\n",
      "\n",
      "-- Operator 'GlobalAveragePool' (name: 'GlobalAveragePool_97') --\n",
      "463 : [1, 1280, 7, 7]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Shape' (name: 'Shape_98') --\n",
      "463 : [1, 1280, 7, 7]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Constant' (name: 'Constant_99') --\n",
      "Attributes:\n",
      "  value: name: \"value\"\n",
      "t {\n",
      "  data_type: 7\n",
      "  raw_data: \"\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      "}\n",
      "type: TENSOR\n",
      " (type: 4)\n",
      "\n",
      "-- Operator 'Gather' (name: 'Gather_100') --\n",
      "465 : [4]\n",
      "466 : []\n",
      "Attributes:\n",
      "  axis: 0\n",
      "\n",
      "-- Operator 'Unsqueeze' (name: 'Unsqueeze_101') --\n",
      "467 : []\n",
      "Attributes:\n",
      "  axes: [0]\n",
      "\n",
      "-- Operator 'Concat' (name: 'Concat_102') --\n",
      "469 : [1]\n",
      "630 : [1]\n",
      "Attributes:\n",
      "  axis: 0\n",
      "\n",
      "-- Operator 'Reshape' (name: 'Reshape_103') --\n",
      "464 : [1, 1280, 1, 1]\n",
      "471 : [2]\n",
      "  No attributes defined\n",
      "\n",
      "-- Operator 'Gemm' (name: 'Gemm_104') --\n",
      "472 : []\n",
      "classifier.1.weight : [1000, 1280]\n",
      "classifier.1.bias : [1000]\n",
      "Attributes:\n",
      "  alpha: 1.0\n",
      "  beta: 1.0\n",
      "  transB: 1\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load('./mobilenetv2-10.onnx')\n",
    "onnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n",
    "\n",
    "# 提取名稱字典（提高查找效率）\n",
    "input_dict = {k.name: idx for idx, k in enumerate(onnx_model.graph.input)}\n",
    "initializer_dict = {k.name: idx for idx, k in enumerate(onnx_model.graph.initializer)}\n",
    "value_info_dict = {k.name: idx for idx, k in enumerate(onnx_model.graph.value_info)}\n",
    "\n",
    "\n",
    "# 遍歷所有節點，提取每個 operator 的屬性\n",
    "for node in onnx_model.graph.node:\n",
    "    print(f\"\\n-- Operator '{node.op_type}' (name: '{node.name}') --\")\n",
    "    \n",
    "    # 打印輸入資訊\n",
    "    # print(node)\n",
    "    for input_name in node.input:\n",
    "        if input_name in input_dict:\n",
    "            idx = input_dict[input_name]\n",
    "            dims, size = get_size(onnx_model.graph.input[idx].type.tensor_type.shape)\n",
    "            # print(f\"Input {input_name} (data) has {size} elements, dims = {dims}\")\n",
    "            print(f\"{input_name} : {dims}\")\n",
    "        elif input_name in initializer_dict:\n",
    "            idx = initializer_dict[input_name]\n",
    "            dims, size = get_size2(onnx_model.graph.initializer[idx])\n",
    "            # print(f\"Input {input_name} (weight/bias) has {size} elements, dims = {dims}\")\n",
    "            print(f\"{input_name} : {dims}\")\n",
    "        elif input_name in value_info_dict:\n",
    "            idx = value_info_dict[input_name]\n",
    "            dims, size = get_size(onnx_model.graph.value_info[idx].type.tensor_type.shape)\n",
    "            # print(f\"Input {input_name} (intermediate) has {size} elements, dims = {dims}\")\n",
    "            print(f\"{input_name} : {dims}\")\n",
    "        else:\n",
    "            print(f\"Input {input_name} not found in input, initializer, or value_info\")\n",
    "    \n",
    "    # 打印屬性值\n",
    "    if node.attribute:\n",
    "        print(\"Attributes:\")\n",
    "        for attr in node.attribute:\n",
    "            if attr.type == onnx.AttributeProto.INT:\n",
    "                print(f\"  {attr.name}: {attr.i}\")\n",
    "            elif attr.type == onnx.AttributeProto.INTS:\n",
    "                print(f\"  {attr.name}: {list(attr.ints)}\")\n",
    "            elif attr.type == onnx.AttributeProto.FLOAT:\n",
    "                print(f\"  {attr.name}: {attr.f}\")\n",
    "            elif attr.type == onnx.AttributeProto.STRING:\n",
    "                # print(\"print(attr.type)=\"+attr.type)\n",
    "                print(f\"  {attr.name}: {attr.s.decode('utf-8')}\")\n",
    "            else:\n",
    "                print(f\"  {attr.name}: {attr} (type: {attr.type})\")\n",
    "    else:\n",
    "        print(\"  No attributes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50526577-8909-40d9-8639-4872d22fd8fa",
   "metadata": {},
   "source": [
    "## 2-2-2. Data bandwidth requirement\n",
    "\n",
    "- Assuming all required data is moved only once, how much data bandwidth is required to do model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "52c414af-9b2b-4923-882d-fc2ced0d75d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input input (data) has 150528 elements\n",
      "Input 475 (weight/bias) has 864 elements\n",
      "Input 476 (weight/bias) has 32 elements\n",
      "Input 474 (intermediate) has 401408 elements\n",
      "Input 317 (intermediate) has 401408 elements\n",
      "Input 478 (weight/bias) has 288 elements\n",
      "Input 479 (weight/bias) has 32 elements\n",
      "Input 477 (intermediate) has 401408 elements\n",
      "Input 320 (intermediate) has 401408 elements\n",
      "Input 481 (weight/bias) has 512 elements\n",
      "Input 482 (weight/bias) has 16 elements\n",
      "Input 480 (intermediate) has 200704 elements\n",
      "Input 484 (weight/bias) has 1536 elements\n",
      "Input 485 (weight/bias) has 96 elements\n",
      "Input 483 (intermediate) has 1204224 elements\n",
      "Input 325 (intermediate) has 1204224 elements\n",
      "Input 487 (weight/bias) has 864 elements\n",
      "Input 488 (weight/bias) has 96 elements\n",
      "Input 486 (intermediate) has 301056 elements\n",
      "Input 328 (intermediate) has 301056 elements\n",
      "Input 490 (weight/bias) has 2304 elements\n",
      "Input 491 (weight/bias) has 24 elements\n",
      "Input 489 (intermediate) has 75264 elements\n",
      "Input 493 (weight/bias) has 3456 elements\n",
      "Input 494 (weight/bias) has 144 elements\n",
      "Input 492 (intermediate) has 451584 elements\n",
      "Input 333 (intermediate) has 451584 elements\n",
      "Input 496 (weight/bias) has 1296 elements\n",
      "Input 497 (weight/bias) has 144 elements\n",
      "Input 495 (intermediate) has 451584 elements\n",
      "Input 336 (intermediate) has 451584 elements\n",
      "Input 499 (weight/bias) has 3456 elements\n",
      "Input 500 (weight/bias) has 24 elements\n",
      "Input 489 (intermediate) has 75264 elements\n",
      "Input 498 (intermediate) has 75264 elements\n",
      "Input 339 (intermediate) has 75264 elements\n",
      "Input 502 (weight/bias) has 3456 elements\n",
      "Input 503 (weight/bias) has 144 elements\n",
      "Input 501 (intermediate) has 451584 elements\n",
      "Input 342 (intermediate) has 451584 elements\n",
      "Input 505 (weight/bias) has 1296 elements\n",
      "Input 506 (weight/bias) has 144 elements\n",
      "Input 504 (intermediate) has 112896 elements\n",
      "Input 345 (intermediate) has 112896 elements\n",
      "Input 508 (weight/bias) has 4608 elements\n",
      "Input 509 (weight/bias) has 32 elements\n",
      "Input 507 (intermediate) has 25088 elements\n",
      "Input 511 (weight/bias) has 6144 elements\n",
      "Input 512 (weight/bias) has 192 elements\n",
      "Input 510 (intermediate) has 150528 elements\n",
      "Input 350 (intermediate) has 150528 elements\n",
      "Input 514 (weight/bias) has 1728 elements\n",
      "Input 515 (weight/bias) has 192 elements\n",
      "Input 513 (intermediate) has 150528 elements\n",
      "Input 353 (intermediate) has 150528 elements\n",
      "Input 517 (weight/bias) has 6144 elements\n",
      "Input 518 (weight/bias) has 32 elements\n",
      "Input 507 (intermediate) has 25088 elements\n",
      "Input 516 (intermediate) has 25088 elements\n",
      "Input 356 (intermediate) has 25088 elements\n",
      "Input 520 (weight/bias) has 6144 elements\n",
      "Input 521 (weight/bias) has 192 elements\n",
      "Input 519 (intermediate) has 150528 elements\n",
      "Input 359 (intermediate) has 150528 elements\n",
      "Input 523 (weight/bias) has 1728 elements\n",
      "Input 524 (weight/bias) has 192 elements\n",
      "Input 522 (intermediate) has 150528 elements\n",
      "Input 362 (intermediate) has 150528 elements\n",
      "Input 526 (weight/bias) has 6144 elements\n",
      "Input 527 (weight/bias) has 32 elements\n",
      "Input 356 (intermediate) has 25088 elements\n",
      "Input 525 (intermediate) has 25088 elements\n",
      "Input 365 (intermediate) has 25088 elements\n",
      "Input 529 (weight/bias) has 6144 elements\n",
      "Input 530 (weight/bias) has 192 elements\n",
      "Input 528 (intermediate) has 150528 elements\n",
      "Input 368 (intermediate) has 150528 elements\n",
      "Input 532 (weight/bias) has 1728 elements\n",
      "Input 533 (weight/bias) has 192 elements\n",
      "Input 531 (intermediate) has 37632 elements\n",
      "Input 371 (intermediate) has 37632 elements\n",
      "Input 535 (weight/bias) has 12288 elements\n",
      "Input 536 (weight/bias) has 64 elements\n",
      "Input 534 (intermediate) has 12544 elements\n",
      "Input 538 (weight/bias) has 24576 elements\n",
      "Input 539 (weight/bias) has 384 elements\n",
      "Input 537 (intermediate) has 75264 elements\n",
      "Input 376 (intermediate) has 75264 elements\n",
      "Input 541 (weight/bias) has 3456 elements\n",
      "Input 542 (weight/bias) has 384 elements\n",
      "Input 540 (intermediate) has 75264 elements\n",
      "Input 379 (intermediate) has 75264 elements\n",
      "Input 544 (weight/bias) has 24576 elements\n",
      "Input 545 (weight/bias) has 64 elements\n",
      "Input 534 (intermediate) has 12544 elements\n",
      "Input 543 (intermediate) has 12544 elements\n",
      "Input 382 (intermediate) has 12544 elements\n",
      "Input 547 (weight/bias) has 24576 elements\n",
      "Input 548 (weight/bias) has 384 elements\n",
      "Input 546 (intermediate) has 75264 elements\n",
      "Input 385 (intermediate) has 75264 elements\n",
      "Input 550 (weight/bias) has 3456 elements\n",
      "Input 551 (weight/bias) has 384 elements\n",
      "Input 549 (intermediate) has 75264 elements\n",
      "Input 388 (intermediate) has 75264 elements\n",
      "Input 553 (weight/bias) has 24576 elements\n",
      "Input 554 (weight/bias) has 64 elements\n",
      "Input 382 (intermediate) has 12544 elements\n",
      "Input 552 (intermediate) has 12544 elements\n",
      "Input 391 (intermediate) has 12544 elements\n",
      "Input 556 (weight/bias) has 24576 elements\n",
      "Input 557 (weight/bias) has 384 elements\n",
      "Input 555 (intermediate) has 75264 elements\n",
      "Input 394 (intermediate) has 75264 elements\n",
      "Input 559 (weight/bias) has 3456 elements\n",
      "Input 560 (weight/bias) has 384 elements\n",
      "Input 558 (intermediate) has 75264 elements\n",
      "Input 397 (intermediate) has 75264 elements\n",
      "Input 562 (weight/bias) has 24576 elements\n",
      "Input 563 (weight/bias) has 64 elements\n",
      "Input 391 (intermediate) has 12544 elements\n",
      "Input 561 (intermediate) has 12544 elements\n",
      "Input 400 (intermediate) has 12544 elements\n",
      "Input 565 (weight/bias) has 24576 elements\n",
      "Input 566 (weight/bias) has 384 elements\n",
      "Input 564 (intermediate) has 75264 elements\n",
      "Input 403 (intermediate) has 75264 elements\n",
      "Input 568 (weight/bias) has 3456 elements\n",
      "Input 569 (weight/bias) has 384 elements\n",
      "Input 567 (intermediate) has 75264 elements\n",
      "Input 406 (intermediate) has 75264 elements\n",
      "Input 571 (weight/bias) has 36864 elements\n",
      "Input 572 (weight/bias) has 96 elements\n",
      "Input 570 (intermediate) has 18816 elements\n",
      "Input 574 (weight/bias) has 55296 elements\n",
      "Input 575 (weight/bias) has 576 elements\n",
      "Input 573 (intermediate) has 112896 elements\n",
      "Input 411 (intermediate) has 112896 elements\n",
      "Input 577 (weight/bias) has 5184 elements\n",
      "Input 578 (weight/bias) has 576 elements\n",
      "Input 576 (intermediate) has 112896 elements\n",
      "Input 414 (intermediate) has 112896 elements\n",
      "Input 580 (weight/bias) has 55296 elements\n",
      "Input 581 (weight/bias) has 96 elements\n",
      "Input 570 (intermediate) has 18816 elements\n",
      "Input 579 (intermediate) has 18816 elements\n",
      "Input 417 (intermediate) has 18816 elements\n",
      "Input 583 (weight/bias) has 55296 elements\n",
      "Input 584 (weight/bias) has 576 elements\n",
      "Input 582 (intermediate) has 112896 elements\n",
      "Input 420 (intermediate) has 112896 elements\n",
      "Input 586 (weight/bias) has 5184 elements\n",
      "Input 587 (weight/bias) has 576 elements\n",
      "Input 585 (intermediate) has 112896 elements\n",
      "Input 423 (intermediate) has 112896 elements\n",
      "Input 589 (weight/bias) has 55296 elements\n",
      "Input 590 (weight/bias) has 96 elements\n",
      "Input 417 (intermediate) has 18816 elements\n",
      "Input 588 (intermediate) has 18816 elements\n",
      "Input 426 (intermediate) has 18816 elements\n",
      "Input 592 (weight/bias) has 55296 elements\n",
      "Input 593 (weight/bias) has 576 elements\n",
      "Input 591 (intermediate) has 112896 elements\n",
      "Input 429 (intermediate) has 112896 elements\n",
      "Input 595 (weight/bias) has 5184 elements\n",
      "Input 596 (weight/bias) has 576 elements\n",
      "Input 594 (intermediate) has 28224 elements\n",
      "Input 432 (intermediate) has 28224 elements\n",
      "Input 598 (weight/bias) has 92160 elements\n",
      "Input 599 (weight/bias) has 160 elements\n",
      "Input 597 (intermediate) has 7840 elements\n",
      "Input 601 (weight/bias) has 153600 elements\n",
      "Input 602 (weight/bias) has 960 elements\n",
      "Input 600 (intermediate) has 47040 elements\n",
      "Input 437 (intermediate) has 47040 elements\n",
      "Input 604 (weight/bias) has 8640 elements\n",
      "Input 605 (weight/bias) has 960 elements\n",
      "Input 603 (intermediate) has 47040 elements\n",
      "Input 440 (intermediate) has 47040 elements\n",
      "Input 607 (weight/bias) has 153600 elements\n",
      "Input 608 (weight/bias) has 160 elements\n",
      "Input 597 (intermediate) has 7840 elements\n",
      "Input 606 (intermediate) has 7840 elements\n",
      "Input 443 (intermediate) has 7840 elements\n",
      "Input 610 (weight/bias) has 153600 elements\n",
      "Input 611 (weight/bias) has 960 elements\n",
      "Input 609 (intermediate) has 47040 elements\n",
      "Input 446 (intermediate) has 47040 elements\n",
      "Input 613 (weight/bias) has 8640 elements\n",
      "Input 614 (weight/bias) has 960 elements\n",
      "Input 612 (intermediate) has 47040 elements\n",
      "Input 449 (intermediate) has 47040 elements\n",
      "Input 616 (weight/bias) has 153600 elements\n",
      "Input 617 (weight/bias) has 160 elements\n",
      "Input 443 (intermediate) has 7840 elements\n",
      "Input 615 (intermediate) has 7840 elements\n",
      "Input 452 (intermediate) has 7840 elements\n",
      "Input 619 (weight/bias) has 153600 elements\n",
      "Input 620 (weight/bias) has 960 elements\n",
      "Input 618 (intermediate) has 47040 elements\n",
      "Input 455 (intermediate) has 47040 elements\n",
      "Input 622 (weight/bias) has 8640 elements\n",
      "Input 623 (weight/bias) has 960 elements\n",
      "Input 621 (intermediate) has 47040 elements\n",
      "Input 458 (intermediate) has 47040 elements\n",
      "Input 625 (weight/bias) has 307200 elements\n",
      "Input 626 (weight/bias) has 320 elements\n",
      "Input 624 (intermediate) has 15680 elements\n",
      "Input 628 (weight/bias) has 409600 elements\n",
      "Input 629 (weight/bias) has 1280 elements\n",
      "Input 627 (intermediate) has 62720 elements\n",
      "Input 463 (intermediate) has 62720 elements\n",
      "Input 463 (intermediate) has 62720 elements\n",
      "Input 465 (intermediate) has 4 elements\n",
      "Input 466 (intermediate) has 1 elements\n",
      "Input 467 (intermediate) has 1 elements\n",
      "Input 469 (intermediate) has 1 elements\n",
      "Input 630 (weight/bias) has 1 elements\n",
      "Input 464 (intermediate) has 1280 elements\n",
      "Input 471 (intermediate) has 2 elements\n",
      "Input 472 (intermediate) has 1 elements\n",
      "Input classifier.1.weight (weight/bias) has 1280000 elements\n",
      "Input classifier.1.bias (weight/bias) has 1000 elements\n",
      "Output output (intermediate) has 1000 elements\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(\"mobilenetv2-10.onnx\")\n",
    "onnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n",
    "\n",
    "# 提取名稱字典\n",
    "input_dict = {k.name: idx for idx, k in enumerate(onnx_model.graph.input)}\n",
    "initializer_dict = {k.name: idx for idx, k in enumerate(onnx_model.graph.initializer)}\n",
    "value_info_dict = {k.name: idx for idx, k in enumerate(onnx_model.graph.value_info)}\n",
    "output_dict = {k.name: idx for idx, k in enumerate(onnx_model.graph.output)}\n",
    "\n",
    "# 假設數據類型為 float32（4 字節）\n",
    "dtype_size = 4  # float32 每元素 4 字節\n",
    "total_bytes = 0\n",
    "# 遍歷所有節點，提取每個 operator 的屬性\n",
    "for node in onnx_model.graph.node:\n",
    "    # print(f\"\\n-- Operator '{node.op_type}' (name: '{node.name}') --\")\n",
    "    \n",
    "    # 打印輸入資訊\n",
    "    # print(node)\n",
    "    for input_name in node.input:\n",
    "        if input_name in input_dict:\n",
    "            idx = input_dict[input_name]\n",
    "            dims, size = get_size(onnx_model.graph.input[idx].type.tensor_type.shape)\n",
    "            print(f\"Input {input_name} (data) has {size} elements\")\n",
    "            total_bytes += size *dtype_size\n",
    "        elif input_name in initializer_dict:\n",
    "            idx = initializer_dict[input_name]\n",
    "            dims, size = get_size2(onnx_model.graph.initializer[idx])\n",
    "            print(f\"Input {input_name} (weight/bias) has {size} elements\")\n",
    "            total_bytes += size*dtype_size\n",
    "        elif input_name in value_info_dict:\n",
    "            idx = value_info_dict[input_name]\n",
    "            dims, size = get_size(onnx_model.graph.value_info[idx].type.tensor_type.shape)\n",
    "            print(f\"Input {input_name} (intermediate) has {size} elements\")\n",
    "            total_bytes += size*dtype_size\n",
    "        elif input_name in output_dict:\n",
    "            dims, size = get_size(onnx_model.graph.value_info[idx].type.tensor_type.shape)\n",
    "            print(f\"Input {input_name} (intermediate) has {size} elements\")\n",
    "            total_bytes += size*dtype_size\n",
    "        else:\n",
    "            print(f\"Input {input_name} not found in input, initializer, or value_info\")\n",
    "for output_tensor in onnx_model.graph.output:\n",
    "    dims, size = get_size(output_tensor.type.tensor_type.shape)\n",
    "    print(f\"Output {output_tensor.name} (intermediate) has {size} elements\")\n",
    "    total_bytes += size*dtype_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e17135dc-42b1-48b0-b9b6-5fad8e47b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.680108 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{total_bytes/float(1000000)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc7d7074-9e84-4de6-868e-d0b8f4a052c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inference complete ...\n",
      "start\n",
      "Conv [1, 3, 224, 224] 602112\n",
      "Conv [32, 3, 3, 3] 3456\n",
      "Conv [32] 128\n",
      "Clip [1, 32, 112, 112] 1605632\n",
      "Conv [1, 32, 112, 112] 1605632\n",
      "Conv [32, 1, 3, 3] 1152\n",
      "Conv [32] 128\n",
      "Clip [1, 32, 112, 112] 1605632\n",
      "Conv [1, 32, 112, 112] 1605632\n",
      "Conv [16, 32, 1, 1] 2048\n",
      "Conv [16] 64\n",
      "Conv [1, 16, 112, 112] 802816\n",
      "Conv [96, 16, 1, 1] 6144\n",
      "Conv [96] 384\n",
      "Clip [1, 96, 112, 112] 4816896\n",
      "Conv [1, 96, 112, 112] 4816896\n",
      "Conv [96, 1, 3, 3] 3456\n",
      "Conv [96] 384\n",
      "Clip [1, 96, 56, 56] 1204224\n",
      "Conv [1, 96, 56, 56] 1204224\n",
      "Conv [24, 96, 1, 1] 9216\n",
      "Conv [24] 96\n",
      "Conv [1, 24, 56, 56] 301056\n",
      "Conv [144, 24, 1, 1] 13824\n",
      "Conv [144] 576\n",
      "Clip [1, 144, 56, 56] 1806336\n",
      "Conv [1, 144, 56, 56] 1806336\n",
      "Conv [144, 1, 3, 3] 5184\n",
      "Conv [144] 576\n",
      "Clip [1, 144, 56, 56] 1806336\n",
      "Conv [1, 144, 56, 56] 1806336\n",
      "Conv [24, 144, 1, 1] 13824\n",
      "Conv [24] 96\n",
      "Add [1, 24, 56, 56] 301056\n",
      "Add [1, 24, 56, 56] 301056\n",
      "Conv [1, 24, 56, 56] 301056\n",
      "Conv [144, 24, 1, 1] 13824\n",
      "Conv [144] 576\n",
      "Clip [1, 144, 56, 56] 1806336\n",
      "Conv [1, 144, 56, 56] 1806336\n",
      "Conv [144, 1, 3, 3] 5184\n",
      "Conv [144] 576\n",
      "Clip [1, 144, 28, 28] 451584\n",
      "Conv [1, 144, 28, 28] 451584\n",
      "Conv [32, 144, 1, 1] 18432\n",
      "Conv [32] 128\n",
      "Conv [1, 32, 28, 28] 100352\n",
      "Conv [192, 32, 1, 1] 24576\n",
      "Conv [192] 768\n",
      "Clip [1, 192, 28, 28] 602112\n",
      "Conv [1, 192, 28, 28] 602112\n",
      "Conv [192, 1, 3, 3] 6912\n",
      "Conv [192] 768\n",
      "Clip [1, 192, 28, 28] 602112\n",
      "Conv [1, 192, 28, 28] 602112\n",
      "Conv [32, 192, 1, 1] 24576\n",
      "Conv [32] 128\n",
      "Add [1, 32, 28, 28] 100352\n",
      "Add [1, 32, 28, 28] 100352\n",
      "Conv [1, 32, 28, 28] 100352\n",
      "Conv [192, 32, 1, 1] 24576\n",
      "Conv [192] 768\n",
      "Clip [1, 192, 28, 28] 602112\n",
      "Conv [1, 192, 28, 28] 602112\n",
      "Conv [192, 1, 3, 3] 6912\n",
      "Conv [192] 768\n",
      "Clip [1, 192, 28, 28] 602112\n",
      "Conv [1, 192, 28, 28] 602112\n",
      "Conv [32, 192, 1, 1] 24576\n",
      "Conv [32] 128\n",
      "Add [1, 32, 28, 28] 100352\n",
      "Add [1, 32, 28, 28] 100352\n",
      "Conv [1, 32, 28, 28] 100352\n",
      "Conv [192, 32, 1, 1] 24576\n",
      "Conv [192] 768\n",
      "Clip [1, 192, 28, 28] 602112\n",
      "Conv [1, 192, 28, 28] 602112\n",
      "Conv [192, 1, 3, 3] 6912\n",
      "Conv [192] 768\n",
      "Clip [1, 192, 14, 14] 150528\n",
      "Conv [1, 192, 14, 14] 150528\n",
      "Conv [64, 192, 1, 1] 49152\n",
      "Conv [64] 256\n",
      "Conv [1, 64, 14, 14] 50176\n",
      "Conv [384, 64, 1, 1] 98304\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [384, 1, 3, 3] 13824\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [64, 384, 1, 1] 98304\n",
      "Conv [64] 256\n",
      "Add [1, 64, 14, 14] 50176\n",
      "Add [1, 64, 14, 14] 50176\n",
      "Conv [1, 64, 14, 14] 50176\n",
      "Conv [384, 64, 1, 1] 98304\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [384, 1, 3, 3] 13824\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [64, 384, 1, 1] 98304\n",
      "Conv [64] 256\n",
      "Add [1, 64, 14, 14] 50176\n",
      "Add [1, 64, 14, 14] 50176\n",
      "Conv [1, 64, 14, 14] 50176\n",
      "Conv [384, 64, 1, 1] 98304\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [384, 1, 3, 3] 13824\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [64, 384, 1, 1] 98304\n",
      "Conv [64] 256\n",
      "Add [1, 64, 14, 14] 50176\n",
      "Add [1, 64, 14, 14] 50176\n",
      "Conv [1, 64, 14, 14] 50176\n",
      "Conv [384, 64, 1, 1] 98304\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [384, 1, 3, 3] 13824\n",
      "Conv [384] 1536\n",
      "Clip [1, 384, 14, 14] 301056\n",
      "Conv [1, 384, 14, 14] 301056\n",
      "Conv [96, 384, 1, 1] 147456\n",
      "Conv [96] 384\n",
      "Conv [1, 96, 14, 14] 75264\n",
      "Conv [576, 96, 1, 1] 221184\n",
      "Conv [576] 2304\n",
      "Clip [1, 576, 14, 14] 451584\n",
      "Conv [1, 576, 14, 14] 451584\n",
      "Conv [576, 1, 3, 3] 20736\n",
      "Conv [576] 2304\n",
      "Clip [1, 576, 14, 14] 451584\n",
      "Conv [1, 576, 14, 14] 451584\n",
      "Conv [96, 576, 1, 1] 221184\n",
      "Conv [96] 384\n",
      "Add [1, 96, 14, 14] 75264\n",
      "Add [1, 96, 14, 14] 75264\n",
      "Conv [1, 96, 14, 14] 75264\n",
      "Conv [576, 96, 1, 1] 221184\n",
      "Conv [576] 2304\n",
      "Clip [1, 576, 14, 14] 451584\n",
      "Conv [1, 576, 14, 14] 451584\n",
      "Conv [576, 1, 3, 3] 20736\n",
      "Conv [576] 2304\n",
      "Clip [1, 576, 14, 14] 451584\n",
      "Conv [1, 576, 14, 14] 451584\n",
      "Conv [96, 576, 1, 1] 221184\n",
      "Conv [96] 384\n",
      "Add [1, 96, 14, 14] 75264\n",
      "Add [1, 96, 14, 14] 75264\n",
      "Conv [1, 96, 14, 14] 75264\n",
      "Conv [576, 96, 1, 1] 221184\n",
      "Conv [576] 2304\n",
      "Clip [1, 576, 14, 14] 451584\n",
      "Conv [1, 576, 14, 14] 451584\n",
      "Conv [576, 1, 3, 3] 20736\n",
      "Conv [576] 2304\n",
      "Clip [1, 576, 7, 7] 112896\n",
      "Conv [1, 576, 7, 7] 112896\n",
      "Conv [160, 576, 1, 1] 368640\n",
      "Conv [160] 640\n",
      "Conv [1, 160, 7, 7] 31360\n",
      "Conv [960, 160, 1, 1] 614400\n",
      "Conv [960] 3840\n",
      "Clip [1, 960, 7, 7] 188160\n",
      "Conv [1, 960, 7, 7] 188160\n",
      "Conv [960, 1, 3, 3] 34560\n",
      "Conv [960] 3840\n",
      "Clip [1, 960, 7, 7] 188160\n",
      "Conv [1, 960, 7, 7] 188160\n",
      "Conv [160, 960, 1, 1] 614400\n",
      "Conv [160] 640\n",
      "Add [1, 160, 7, 7] 31360\n",
      "Add [1, 160, 7, 7] 31360\n",
      "Conv [1, 160, 7, 7] 31360\n",
      "Conv [960, 160, 1, 1] 614400\n",
      "Conv [960] 3840\n",
      "Clip [1, 960, 7, 7] 188160\n",
      "Conv [1, 960, 7, 7] 188160\n",
      "Conv [960, 1, 3, 3] 34560\n",
      "Conv [960] 3840\n",
      "Clip [1, 960, 7, 7] 188160\n",
      "Conv [1, 960, 7, 7] 188160\n",
      "Conv [160, 960, 1, 1] 614400\n",
      "Conv [160] 640\n",
      "Add [1, 160, 7, 7] 31360\n",
      "Add [1, 160, 7, 7] 31360\n",
      "Conv [1, 160, 7, 7] 31360\n",
      "Conv [960, 160, 1, 1] 614400\n",
      "Conv [960] 3840\n",
      "Clip [1, 960, 7, 7] 188160\n",
      "Conv [1, 960, 7, 7] 188160\n",
      "Conv [960, 1, 3, 3] 34560\n",
      "Conv [960] 3840\n",
      "Clip [1, 960, 7, 7] 188160\n",
      "Conv [1, 960, 7, 7] 188160\n",
      "Conv [320, 960, 1, 1] 1228800\n",
      "Conv [320] 1280\n",
      "Conv [1, 320, 7, 7] 62720\n",
      "Conv [1280, 320, 1, 1] 1638400\n",
      "Conv [1280] 5120\n",
      "Clip [1, 1280, 7, 7] 250880\n",
      "GlobalAveragePool [1, 1280, 7, 7] 250880\n",
      "Shape [1, 1280, 7, 7] 250880\n",
      "Gather [4] 32\n",
      "Gather [] 8\n",
      "Unsqueeze [] 8\n",
      "Concat [1] 8\n",
      "Concat [1] 8\n",
      "Reshape [1, 1280, 1, 1] 5120\n",
      "Reshape [2] 16\n",
      "Gemm [] 4\n",
      "Gemm [1000, 1280] 5120000\n",
      "Gemm [1000] 4000\n",
      "layer                   read_bw    write_bw    total_bw\n",
      "--------------------  ---------  ----------  ----------\n",
      "Gemm_104                5124004        4000     5128004\n",
      "Conv_7                  4820736     1204224     6024960\n",
      "Clip_6                  4816896     4816896     9633792\n",
      "Conv_14                 1820256      301056     2121312\n",
      "Conv_12                 1812096     1806336     3618432\n",
      "Conv_18                 1812096      451584     2263680\n",
      "Clip_11                 1806336     1806336     3612672\n",
      "Clip_13                 1806336     1806336     3612672\n",
      "Clip_17                 1806336     1806336     3612672\n",
      "Conv_95                 1706240      250880     1957120\n",
      "Conv_4                  1607744      802816     2410560\n",
      "Conv_2                  1606912     1605632     3212544\n",
      "Clip_1                  1605632     1605632     3211264\n",
      "Clip_3                  1605632     1605632     3211264\n",
      "Conv_94                 1418240       62720     1480960\n",
      "Conv_9                  1213536      301056     1514592\n",
      "Clip_8                  1204224     1204224     2408448\n",
      "Conv_5                   809344     4816896     5626240\n",
      "Conv_82                  803200       31360      834560\n",
      "Conv_88                  803200       31360      834560\n",
      "Conv_65                  673152       75264      748416\n",
      "Conv_71                  673152       75264      748416\n",
      "Conv_78                  649600      188160      837760\n",
      "Conv_84                  649600      188160      837760\n",
      "Conv_90                  649600      188160      837760\n",
      "Conv_25                  626816      100352      727168\n",
      "Conv_31                  626816      100352      727168\n",
      "Conv_23                  609792      602112     1211904\n",
      "Conv_29                  609792      602112     1211904\n",
      "Conv_35                  609792      150528      760320\n",
      "Conv_0                   605696     1605632     2211328\n",
      "Add_15                   602112      301056      903168\n",
      "Clip_22                  602112      602112     1204224\n",
      "Clip_24                  602112      602112     1204224\n",
      "Clip_28                  602112      602112     1204224\n",
      "Clip_30                  602112      602112     1204224\n",
      "Clip_34                  602112      602112     1204224\n",
      "Conv_77                  482176       31360      513536\n",
      "Conv_63                  474624      451584      926208\n",
      "Conv_69                  474624      451584      926208\n",
      "Conv_75                  474624      112896      587520\n",
      "Conv_20                  470144      100352      570496\n",
      "Clip_19                  451584      451584      903168\n",
      "Clip_62                  451584      451584      903168\n",
      "Clip_64                  451584      451584      903168\n",
      "Clip_68                  451584      451584      903168\n",
      "Clip_70                  451584      451584      903168\n",
      "Clip_74                  451584      451584      903168\n",
      "Conv_60                  448896       75264      524160\n",
      "Conv_42                  399616       50176      449792\n",
      "Conv_48                  399616       50176      449792\n",
      "Conv_54                  399616       50176      449792\n",
      "Conv_40                  316416      301056      617472\n",
      "Conv_46                  316416      301056      617472\n",
      "Conv_52                  316416      301056      617472\n",
      "Conv_58                  316416      301056      617472\n",
      "Conv_10                  315456     1806336     2121792\n",
      "Conv_16                  315456     1806336     2121792\n",
      "Clip_39                  301056      301056      602112\n",
      "Clip_41                  301056      301056      602112\n",
      "Clip_45                  301056      301056      602112\n",
      "Clip_47                  301056      301056      602112\n",
      "Clip_51                  301056      301056      602112\n",
      "Clip_53                  301056      301056      602112\n",
      "Clip_57                  301056      301056      602112\n",
      "Clip_59                  301056      301056      602112\n",
      "Conv_61                  298752      451584      750336\n",
      "Conv_67                  298752      451584      750336\n",
      "Conv_73                  298752      451584      750336\n",
      "Clip_96                  250880      250880      501760\n",
      "GlobalAveragePool_97     250880        5120      256000\n",
      "Shape_98                 250880          32      250912\n",
      "Conv_80                  226560      188160      414720\n",
      "Conv_86                  226560      188160      414720\n",
      "Conv_92                  226560      188160      414720\n",
      "Add_26                   200704      100352      301056\n",
      "Add_32                   200704      100352      301056\n",
      "Conv_37                  199936       50176      250112\n",
      "Clip_79                  188160      188160      376320\n",
      "Clip_81                  188160      188160      376320\n",
      "Clip_85                  188160      188160      376320\n",
      "Clip_87                  188160      188160      376320\n",
      "Clip_91                  188160      188160      376320\n",
      "Clip_93                  188160      188160      376320\n",
      "Clip_36                  150528      150528      301056\n",
      "Add_66                   150528       75264      225792\n",
      "Add_72                   150528       75264      225792\n",
      "Conv_38                  150016      301056      451072\n",
      "Conv_44                  150016      301056      451072\n",
      "Conv_50                  150016      301056      451072\n",
      "Conv_56                  150016      301056      451072\n",
      "Conv_21                  125696      602112      727808\n",
      "Conv_27                  125696      602112      727808\n",
      "Conv_33                  125696      602112      727808\n",
      "Clip_76                  112896      112896      225792\n",
      "Add_43                   100352       50176      150528\n",
      "Add_49                   100352       50176      150528\n",
      "Add_55                   100352       50176      150528\n",
      "Add_83                    62720       31360       94080\n",
      "Add_89                    62720       31360       94080\n",
      "Reshape_103                5136           4        5140\n",
      "Gather_100                   40           8          48\n",
      "Concat_102                   16          16          32\n",
      "Unsqueeze_101                 8           8          16\n",
      "Constant_99                   0           8           8\n",
      "====================================================================================\n",
      "\n",
      "The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \n",
      " 119686496 (bytes)\n",
      " 119.686496 (MB)\n",
      "\n",
      "op_name    unfound_tensor    op_type\n",
      "---------  ----------------  ---------\n",
      "====================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import shape_inference\n",
    "from os import path\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from onnx import onnx_ml_pb2 as xpb2\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(\"./mobilenetv2-10.onnx\", load_external_data=False)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "inferred_model = shape_inference.infer_shapes(onnx_model)\n",
    "print('shape inference complete ...')\n",
    "\n",
    "def _parse_element(elem: xpb2.ValueInfoProto):\n",
    "    name = getattr(elem, 'name', \"None\")\n",
    "    data_type = \"NA\"\n",
    "    shape_str = \"NA\"\n",
    "    etype = getattr(elem, 'type', False)\n",
    "    if etype:\n",
    "        ttype = getattr(etype, 'tensor_type', False)\n",
    "        if ttype:\n",
    "            data_type = getattr(ttype, 'elem_type', 0)\n",
    "            shape = getattr(elem.type.tensor_type, \"shape\", False)\n",
    "            if shape:\n",
    "                shape_str = \"[\"\n",
    "                dims = getattr(shape, 'dim', [])\n",
    "                # print(dims)\n",
    "                for dim in dims:\n",
    "                    vals = getattr(dim, 'dim_value', \"?\")\n",
    "                    # print(vals)\n",
    "                    if vals == 0: vals=1\n",
    "                    shape_str += (str(vals) + \",\")\n",
    "                shape_str = shape_str.rstrip(\",\")\n",
    "                shape_str += \"]\"\n",
    "    return name, data_type, shape_str\n",
    "\n",
    "def get_valueproto_or_tensorproto_by_name(name: str, graph: xpb2.GraphProto):\n",
    "    for i, node in enumerate(inferred_model.graph.node):\n",
    "            if node.name == \"\":\n",
    "                inferred_model.graph.node[i].name = str(i)\n",
    "    input_nlist = [k.name for k in graph.input]\n",
    "    initializer_nlist = [k.name for k in graph.initializer]\n",
    "    value_info_nlist = [k.name for k in graph.value_info]\n",
    "    output_nlist = [k.name for k in graph.output]\n",
    "\n",
    "    # get tensor data\n",
    "    if name in input_nlist:\n",
    "        idx = input_nlist.index(name)\n",
    "        return graph.input[idx], int(1)\n",
    "    elif name in value_info_nlist:\n",
    "        idx = value_info_nlist.index(name)\n",
    "        return graph.value_info[idx], int(2)\n",
    "    elif name in initializer_nlist:\n",
    "        idx = initializer_nlist.index(name)\n",
    "        return graph.initializer[idx], int(3)\n",
    "    elif name in output_nlist:\n",
    "        idx = output_nlist.index(name)\n",
    "        return graph.output[idx], int(4)\n",
    "    else:\n",
    "        print(\"[ERROR MASSAGE] Can't find the tensor: \", name)\n",
    "        print('input_nlist:\\n', input_nlist)\n",
    "        print('===================')\n",
    "        print('value_info_nlist:\\n', value_info_nlist)\n",
    "        print('===================')\n",
    "        print('initializer_nlist:\\n', initializer_nlist)\n",
    "        print('===================')\n",
    "        print('output_nlist:\\n', output_nlist)\n",
    "        print('===================')\n",
    "        return False, 0\n",
    "\n",
    "def cal_tensor_mem_size(elem_type: str, shape: [int]):\n",
    "    \"\"\" given the element type of the tensor and its shape, and return its memory size.\n",
    "\n",
    "    Utility.\n",
    "\n",
    "    Args:\n",
    "        ttype: the type of the element of the given tensor. format: 'int', ...\n",
    "        shape: the shape of the given tensor. format: [] of int\n",
    "\n",
    "    Returns:\n",
    "        mem_size: int\n",
    "    \"\"\"\n",
    "    # init\n",
    "    mem_size = int(1)\n",
    "    # traverse the list to get the number of the elements\n",
    "    for num in shape:\n",
    "        mem_size *= num\n",
    "    # multiple the size of variable with the number of the elements\n",
    "    # \"FLOAT\": 1,\n",
    "    # \"UINT8\": 2,\n",
    "    # \"INT8\": 3,\n",
    "    # \"UINT16\": 4,\n",
    "    # \"INT16\": 5,\n",
    "    # \"INT32\": 6,\n",
    "    # \"INT64\": 7,\n",
    "    # # \"STRING\" : 8,\n",
    "    # \"BOOL\": 9,\n",
    "    # \"FLOAT16\": 10,\n",
    "    # \"DOUBLE\": 11,\n",
    "    # \"UINT32\": 12,\n",
    "    # \"UINT64\": 13,\n",
    "    # \"COMPLEX64\": 14,\n",
    "    # \"COMPLEX128\": 15\n",
    "    if elem_type == 1:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 2:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 3:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 4:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 5:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 6:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 7:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 9:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 10:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 11:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 12:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 13:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 14:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 15:\n",
    "        mem_size *= 16\n",
    "    else:\n",
    "        print(\"Undefined data type\")\n",
    "\n",
    "    return mem_size\n",
    "\n",
    "\n",
    "\n",
    "def get_bandwidth(graph: xpb2.GraphProto):\n",
    "    try:\n",
    "        mem_BW_list = []\n",
    "        total_mem_BW = 0\n",
    "        unknown_tensor_list = []\n",
    "        # traverse all the nodes\n",
    "        for nodeProto in graph.node:\n",
    "            # init variables\n",
    "            # print(nodeProto)\n",
    "            read_mem_BW_each_layer = 0\n",
    "            write_mem_BW_each_layer = 0\n",
    "            total_each_layer = 0\n",
    "            # traverse all input tensor\n",
    "            for input_name in nodeProto.input:\n",
    "                # get the TensorProto/ValueInfoProto by searching its name\n",
    "                proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                    input_name, graph)\n",
    "                # print(proto,type_Num)\n",
    "                # parse the ValueInfoProto/TensorProto\n",
    "                if proto:\n",
    "                    if type_Num == 3:\n",
    "                        dtype = getattr(proto, 'data_type', False)\n",
    "                        # get the shape of the tensor\n",
    "                        shape = getattr(proto, 'dims', [])\n",
    "                        # print(shape_str)\n",
    "                        \n",
    "                    elif type_Num == 1 or type_Num == 2:\n",
    "                        name, dtype, shape_str = _parse_element(proto)\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        # print(name,shape_str)\n",
    "                        shape = []\n",
    "                        # print(f\"3 = {dtype}, shape={shape}\")\n",
    "                        for dim in shape_str:\n",
    "                            if len(dim) > 0:\n",
    "                                shape.append(int(dim))\n",
    "                    else:\n",
    "                        print(\n",
    "                            '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                            input_name, ' is from a wrong list !')\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is no found !')\n",
    "                    unknown_tensor_list.append(\n",
    "                        (nodeProto.name, input_name, nodeProto.op_type))\n",
    "                # calculate the tensor size in btye\n",
    "                print(nodeProto.op_type,shape,cal_tensor_mem_size(dtype, shape))\n",
    "                read_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "\n",
    "            # traverse all output tensor\n",
    "            for output_name in nodeProto.output:\n",
    "                # get the TensorProto/ValueInfoProto by searching its name\n",
    "                proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                    output_name, graph)\n",
    "                # parse the ValueInfoProto\n",
    "                # print(f\"type_Num={type_Num}\")\n",
    "                if proto:\n",
    "                    if type_Num == 2 or type_Num == 4:\n",
    "                        # name, dtype, shape = utils._parse_ValueInfoProto(proto)\n",
    "                        name, dtype, shape_str = _parse_element(proto)\n",
    "                        # print(name,shape_str)\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        shape = []\n",
    "                        # print(f\"{type_Num} = {dtype}, shape={shape}\")\n",
    "                        for dim in shape_str:\n",
    "                            if len(dim) > 0:\n",
    "                                shape.append(int(dim))\n",
    "                    else:\n",
    "                        print(\n",
    "                            '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                            output_name, ' is from a wrong list !')\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is no found !')\n",
    "                    unknown_tensor_list.append(\n",
    "                        (nodeProto.name, output_name, nodeProto.op_type))\n",
    "                # calculate the tensor size in btye\n",
    "                write_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "\n",
    "            # cal total bw\n",
    "            total_each_layer = read_mem_BW_each_layer + write_mem_BW_each_layer\n",
    "\n",
    "            # store into tuple\n",
    "            temp_tuple = (nodeProto.name, read_mem_BW_each_layer,\n",
    "                        write_mem_BW_each_layer, total_each_layer)\n",
    "            #append it\n",
    "            mem_BW_list.append(temp_tuple)\n",
    "            # accmulate the value\n",
    "            total_mem_BW += total_each_layer\n",
    "\n",
    "        # display the mem_bw of eahc layer\n",
    "        columns = ['layer', 'read_bw', 'write_bw', 'total_bw']\n",
    "        # resort the list\n",
    "        mem_BW_list = sorted(mem_BW_list,\n",
    "                             key=lambda Layer: Layer[1],\n",
    "                             reverse=True)\n",
    "        print(tabulate(mem_BW_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "        # display it\n",
    "        print(\n",
    "            \"The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \\n\",\n",
    "            total_mem_BW, '(bytes)\\n',\n",
    "            float(total_mem_BW) / float(1000000), '(MB)\\n')\n",
    "        # display the unknown tensor\n",
    "        columns = ['op_name', 'unfound_tensor', 'op_type']\n",
    "        print(tabulate(unknown_tensor_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR MASSAGE] Unable to display: \" + str(e))\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "#從這裡開始\n",
    "print(\"start\")\n",
    "get_bandwidth(inferred_model.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22def28-be5f-4e7c-a639-942e60f8f4e7",
   "metadata": {},
   "source": [
    "## 2-2-3. activation memory storage requirement\n",
    "\n",
    "- Assuming the activations are stored to local memory and reuse multiple times, how much local memory storage is required to keep the activations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6132a570-3991-485a-be25-74567f50d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inference complete ...\n",
      "start\n",
      "[1,3,224,224]\n",
      "1 Conv_0 [1, 3, 224, 224] 602112 602112\n",
      "3 Conv_0 [32, 3, 3, 3] 3456 605568\n",
      "3 Conv_0 [32] 128 605696\n",
      "[1,32,112,112]\n",
      "2 Clip_1 [1, 32, 112, 112] 1605632 1605632\n",
      "[1,32,112,112]\n",
      "2 Conv_2 [1, 32, 112, 112] 1605632 1605632\n",
      "3 Conv_2 [32, 1, 3, 3] 1152 1606784\n",
      "3 Conv_2 [32] 128 1606912\n",
      "[1,32,112,112]\n",
      "2 Clip_3 [1, 32, 112, 112] 1605632 1605632\n",
      "[1,32,112,112]\n",
      "2 Conv_4 [1, 32, 112, 112] 1605632 1605632\n",
      "3 Conv_4 [16, 32, 1, 1] 2048 1607680\n",
      "3 Conv_4 [16] 64 1607744\n",
      "[1,16,112,112]\n",
      "2 Conv_5 [1, 16, 112, 112] 802816 802816\n",
      "3 Conv_5 [96, 16, 1, 1] 6144 808960\n",
      "3 Conv_5 [96] 384 809344\n",
      "[1,96,112,112]\n",
      "2 Clip_6 [1, 96, 112, 112] 4816896 4816896\n",
      "[1,96,112,112]\n",
      "2 Conv_7 [1, 96, 112, 112] 4816896 4816896\n",
      "3 Conv_7 [96, 1, 3, 3] 3456 4820352\n",
      "3 Conv_7 [96] 384 4820736\n",
      "[1,96,56,56]\n",
      "2 Clip_8 [1, 96, 56, 56] 1204224 1204224\n",
      "[1,96,56,56]\n",
      "2 Conv_9 [1, 96, 56, 56] 1204224 1204224\n",
      "3 Conv_9 [24, 96, 1, 1] 9216 1213440\n",
      "3 Conv_9 [24] 96 1213536\n",
      "[1,24,56,56]\n",
      "2 Conv_10 [1, 24, 56, 56] 301056 301056\n",
      "3 Conv_10 [144, 24, 1, 1] 13824 314880\n",
      "3 Conv_10 [144] 576 315456\n",
      "[1,144,56,56]\n",
      "2 Clip_11 [1, 144, 56, 56] 1806336 1806336\n",
      "[1,144,56,56]\n",
      "2 Conv_12 [1, 144, 56, 56] 1806336 1806336\n",
      "3 Conv_12 [144, 1, 3, 3] 5184 1811520\n",
      "3 Conv_12 [144] 576 1812096\n",
      "[1,144,56,56]\n",
      "2 Clip_13 [1, 144, 56, 56] 1806336 1806336\n",
      "[1,144,56,56]\n",
      "2 Conv_14 [1, 144, 56, 56] 1806336 1806336\n",
      "3 Conv_14 [24, 144, 1, 1] 13824 1820160\n",
      "3 Conv_14 [24] 96 1820256\n",
      "[1,24,56,56]\n",
      "2 Add_15 [1, 24, 56, 56] 301056 301056\n",
      "[1,24,56,56]\n",
      "2 Add_15 [1, 24, 56, 56] 301056 602112\n",
      "[1,24,56,56]\n",
      "2 Conv_16 [1, 24, 56, 56] 301056 301056\n",
      "3 Conv_16 [144, 24, 1, 1] 13824 314880\n",
      "3 Conv_16 [144] 576 315456\n",
      "[1,144,56,56]\n",
      "2 Clip_17 [1, 144, 56, 56] 1806336 1806336\n",
      "[1,144,56,56]\n",
      "2 Conv_18 [1, 144, 56, 56] 1806336 1806336\n",
      "3 Conv_18 [144, 1, 3, 3] 5184 1811520\n",
      "3 Conv_18 [144] 576 1812096\n",
      "[1,144,28,28]\n",
      "2 Clip_19 [1, 144, 28, 28] 451584 451584\n",
      "[1,144,28,28]\n",
      "2 Conv_20 [1, 144, 28, 28] 451584 451584\n",
      "3 Conv_20 [32, 144, 1, 1] 18432 470016\n",
      "3 Conv_20 [32] 128 470144\n",
      "[1,32,28,28]\n",
      "2 Conv_21 [1, 32, 28, 28] 100352 100352\n",
      "3 Conv_21 [192, 32, 1, 1] 24576 124928\n",
      "3 Conv_21 [192] 768 125696\n",
      "[1,192,28,28]\n",
      "2 Clip_22 [1, 192, 28, 28] 602112 602112\n",
      "[1,192,28,28]\n",
      "2 Conv_23 [1, 192, 28, 28] 602112 602112\n",
      "3 Conv_23 [192, 1, 3, 3] 6912 609024\n",
      "3 Conv_23 [192] 768 609792\n",
      "[1,192,28,28]\n",
      "2 Clip_24 [1, 192, 28, 28] 602112 602112\n",
      "[1,192,28,28]\n",
      "2 Conv_25 [1, 192, 28, 28] 602112 602112\n",
      "3 Conv_25 [32, 192, 1, 1] 24576 626688\n",
      "3 Conv_25 [32] 128 626816\n",
      "[1,32,28,28]\n",
      "2 Add_26 [1, 32, 28, 28] 100352 100352\n",
      "[1,32,28,28]\n",
      "2 Add_26 [1, 32, 28, 28] 100352 200704\n",
      "[1,32,28,28]\n",
      "2 Conv_27 [1, 32, 28, 28] 100352 100352\n",
      "3 Conv_27 [192, 32, 1, 1] 24576 124928\n",
      "3 Conv_27 [192] 768 125696\n",
      "[1,192,28,28]\n",
      "2 Clip_28 [1, 192, 28, 28] 602112 602112\n",
      "[1,192,28,28]\n",
      "2 Conv_29 [1, 192, 28, 28] 602112 602112\n",
      "3 Conv_29 [192, 1, 3, 3] 6912 609024\n",
      "3 Conv_29 [192] 768 609792\n",
      "[1,192,28,28]\n",
      "2 Clip_30 [1, 192, 28, 28] 602112 602112\n",
      "[1,192,28,28]\n",
      "2 Conv_31 [1, 192, 28, 28] 602112 602112\n",
      "3 Conv_31 [32, 192, 1, 1] 24576 626688\n",
      "3 Conv_31 [32] 128 626816\n",
      "[1,32,28,28]\n",
      "2 Add_32 [1, 32, 28, 28] 100352 100352\n",
      "[1,32,28,28]\n",
      "2 Add_32 [1, 32, 28, 28] 100352 200704\n",
      "[1,32,28,28]\n",
      "2 Conv_33 [1, 32, 28, 28] 100352 100352\n",
      "3 Conv_33 [192, 32, 1, 1] 24576 124928\n",
      "3 Conv_33 [192] 768 125696\n",
      "[1,192,28,28]\n",
      "2 Clip_34 [1, 192, 28, 28] 602112 602112\n",
      "[1,192,28,28]\n",
      "2 Conv_35 [1, 192, 28, 28] 602112 602112\n",
      "3 Conv_35 [192, 1, 3, 3] 6912 609024\n",
      "3 Conv_35 [192] 768 609792\n",
      "[1,192,14,14]\n",
      "2 Clip_36 [1, 192, 14, 14] 150528 150528\n",
      "[1,192,14,14]\n",
      "2 Conv_37 [1, 192, 14, 14] 150528 150528\n",
      "3 Conv_37 [64, 192, 1, 1] 49152 199680\n",
      "3 Conv_37 [64] 256 199936\n",
      "[1,64,14,14]\n",
      "2 Conv_38 [1, 64, 14, 14] 50176 50176\n",
      "3 Conv_38 [384, 64, 1, 1] 98304 148480\n",
      "3 Conv_38 [384] 1536 150016\n",
      "[1,384,14,14]\n",
      "2 Clip_39 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_40 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_40 [384, 1, 3, 3] 13824 314880\n",
      "3 Conv_40 [384] 1536 316416\n",
      "[1,384,14,14]\n",
      "2 Clip_41 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_42 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_42 [64, 384, 1, 1] 98304 399360\n",
      "3 Conv_42 [64] 256 399616\n",
      "[1,64,14,14]\n",
      "2 Add_43 [1, 64, 14, 14] 50176 50176\n",
      "[1,64,14,14]\n",
      "2 Add_43 [1, 64, 14, 14] 50176 100352\n",
      "[1,64,14,14]\n",
      "2 Conv_44 [1, 64, 14, 14] 50176 50176\n",
      "3 Conv_44 [384, 64, 1, 1] 98304 148480\n",
      "3 Conv_44 [384] 1536 150016\n",
      "[1,384,14,14]\n",
      "2 Clip_45 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_46 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_46 [384, 1, 3, 3] 13824 314880\n",
      "3 Conv_46 [384] 1536 316416\n",
      "[1,384,14,14]\n",
      "2 Clip_47 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_48 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_48 [64, 384, 1, 1] 98304 399360\n",
      "3 Conv_48 [64] 256 399616\n",
      "[1,64,14,14]\n",
      "2 Add_49 [1, 64, 14, 14] 50176 50176\n",
      "[1,64,14,14]\n",
      "2 Add_49 [1, 64, 14, 14] 50176 100352\n",
      "[1,64,14,14]\n",
      "2 Conv_50 [1, 64, 14, 14] 50176 50176\n",
      "3 Conv_50 [384, 64, 1, 1] 98304 148480\n",
      "3 Conv_50 [384] 1536 150016\n",
      "[1,384,14,14]\n",
      "2 Clip_51 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_52 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_52 [384, 1, 3, 3] 13824 314880\n",
      "3 Conv_52 [384] 1536 316416\n",
      "[1,384,14,14]\n",
      "2 Clip_53 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_54 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_54 [64, 384, 1, 1] 98304 399360\n",
      "3 Conv_54 [64] 256 399616\n",
      "[1,64,14,14]\n",
      "2 Add_55 [1, 64, 14, 14] 50176 50176\n",
      "[1,64,14,14]\n",
      "2 Add_55 [1, 64, 14, 14] 50176 100352\n",
      "[1,64,14,14]\n",
      "2 Conv_56 [1, 64, 14, 14] 50176 50176\n",
      "3 Conv_56 [384, 64, 1, 1] 98304 148480\n",
      "3 Conv_56 [384] 1536 150016\n",
      "[1,384,14,14]\n",
      "2 Clip_57 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_58 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_58 [384, 1, 3, 3] 13824 314880\n",
      "3 Conv_58 [384] 1536 316416\n",
      "[1,384,14,14]\n",
      "2 Clip_59 [1, 384, 14, 14] 301056 301056\n",
      "[1,384,14,14]\n",
      "2 Conv_60 [1, 384, 14, 14] 301056 301056\n",
      "3 Conv_60 [96, 384, 1, 1] 147456 448512\n",
      "3 Conv_60 [96] 384 448896\n",
      "[1,96,14,14]\n",
      "2 Conv_61 [1, 96, 14, 14] 75264 75264\n",
      "3 Conv_61 [576, 96, 1, 1] 221184 296448\n",
      "3 Conv_61 [576] 2304 298752\n",
      "[1,576,14,14]\n",
      "2 Clip_62 [1, 576, 14, 14] 451584 451584\n",
      "[1,576,14,14]\n",
      "2 Conv_63 [1, 576, 14, 14] 451584 451584\n",
      "3 Conv_63 [576, 1, 3, 3] 20736 472320\n",
      "3 Conv_63 [576] 2304 474624\n",
      "[1,576,14,14]\n",
      "2 Clip_64 [1, 576, 14, 14] 451584 451584\n",
      "[1,576,14,14]\n",
      "2 Conv_65 [1, 576, 14, 14] 451584 451584\n",
      "3 Conv_65 [96, 576, 1, 1] 221184 672768\n",
      "3 Conv_65 [96] 384 673152\n",
      "[1,96,14,14]\n",
      "2 Add_66 [1, 96, 14, 14] 75264 75264\n",
      "[1,96,14,14]\n",
      "2 Add_66 [1, 96, 14, 14] 75264 150528\n",
      "[1,96,14,14]\n",
      "2 Conv_67 [1, 96, 14, 14] 75264 75264\n",
      "3 Conv_67 [576, 96, 1, 1] 221184 296448\n",
      "3 Conv_67 [576] 2304 298752\n",
      "[1,576,14,14]\n",
      "2 Clip_68 [1, 576, 14, 14] 451584 451584\n",
      "[1,576,14,14]\n",
      "2 Conv_69 [1, 576, 14, 14] 451584 451584\n",
      "3 Conv_69 [576, 1, 3, 3] 20736 472320\n",
      "3 Conv_69 [576] 2304 474624\n",
      "[1,576,14,14]\n",
      "2 Clip_70 [1, 576, 14, 14] 451584 451584\n",
      "[1,576,14,14]\n",
      "2 Conv_71 [1, 576, 14, 14] 451584 451584\n",
      "3 Conv_71 [96, 576, 1, 1] 221184 672768\n",
      "3 Conv_71 [96] 384 673152\n",
      "[1,96,14,14]\n",
      "2 Add_72 [1, 96, 14, 14] 75264 75264\n",
      "[1,96,14,14]\n",
      "2 Add_72 [1, 96, 14, 14] 75264 150528\n",
      "[1,96,14,14]\n",
      "2 Conv_73 [1, 96, 14, 14] 75264 75264\n",
      "3 Conv_73 [576, 96, 1, 1] 221184 296448\n",
      "3 Conv_73 [576] 2304 298752\n",
      "[1,576,14,14]\n",
      "2 Clip_74 [1, 576, 14, 14] 451584 451584\n",
      "[1,576,14,14]\n",
      "2 Conv_75 [1, 576, 14, 14] 451584 451584\n",
      "3 Conv_75 [576, 1, 3, 3] 20736 472320\n",
      "3 Conv_75 [576] 2304 474624\n",
      "[1,576,7,7]\n",
      "2 Clip_76 [1, 576, 7, 7] 112896 112896\n",
      "[1,576,7,7]\n",
      "2 Conv_77 [1, 576, 7, 7] 112896 112896\n",
      "3 Conv_77 [160, 576, 1, 1] 368640 481536\n",
      "3 Conv_77 [160] 640 482176\n",
      "[1,160,7,7]\n",
      "2 Conv_78 [1, 160, 7, 7] 31360 31360\n",
      "3 Conv_78 [960, 160, 1, 1] 614400 645760\n",
      "3 Conv_78 [960] 3840 649600\n",
      "[1,960,7,7]\n",
      "2 Clip_79 [1, 960, 7, 7] 188160 188160\n",
      "[1,960,7,7]\n",
      "2 Conv_80 [1, 960, 7, 7] 188160 188160\n",
      "3 Conv_80 [960, 1, 3, 3] 34560 222720\n",
      "3 Conv_80 [960] 3840 226560\n",
      "[1,960,7,7]\n",
      "2 Clip_81 [1, 960, 7, 7] 188160 188160\n",
      "[1,960,7,7]\n",
      "2 Conv_82 [1, 960, 7, 7] 188160 188160\n",
      "3 Conv_82 [160, 960, 1, 1] 614400 802560\n",
      "3 Conv_82 [160] 640 803200\n",
      "[1,160,7,7]\n",
      "2 Add_83 [1, 160, 7, 7] 31360 31360\n",
      "[1,160,7,7]\n",
      "2 Add_83 [1, 160, 7, 7] 31360 62720\n",
      "[1,160,7,7]\n",
      "2 Conv_84 [1, 160, 7, 7] 31360 31360\n",
      "3 Conv_84 [960, 160, 1, 1] 614400 645760\n",
      "3 Conv_84 [960] 3840 649600\n",
      "[1,960,7,7]\n",
      "2 Clip_85 [1, 960, 7, 7] 188160 188160\n",
      "[1,960,7,7]\n",
      "2 Conv_86 [1, 960, 7, 7] 188160 188160\n",
      "3 Conv_86 [960, 1, 3, 3] 34560 222720\n",
      "3 Conv_86 [960] 3840 226560\n",
      "[1,960,7,7]\n",
      "2 Clip_87 [1, 960, 7, 7] 188160 188160\n",
      "[1,960,7,7]\n",
      "2 Conv_88 [1, 960, 7, 7] 188160 188160\n",
      "3 Conv_88 [160, 960, 1, 1] 614400 802560\n",
      "3 Conv_88 [160] 640 803200\n",
      "[1,160,7,7]\n",
      "2 Add_89 [1, 160, 7, 7] 31360 31360\n",
      "[1,160,7,7]\n",
      "2 Add_89 [1, 160, 7, 7] 31360 62720\n",
      "[1,160,7,7]\n",
      "2 Conv_90 [1, 160, 7, 7] 31360 31360\n",
      "3 Conv_90 [960, 160, 1, 1] 614400 645760\n",
      "3 Conv_90 [960] 3840 649600\n",
      "[1,960,7,7]\n",
      "2 Clip_91 [1, 960, 7, 7] 188160 188160\n",
      "[1,960,7,7]\n",
      "2 Conv_92 [1, 960, 7, 7] 188160 188160\n",
      "3 Conv_92 [960, 1, 3, 3] 34560 222720\n",
      "3 Conv_92 [960] 3840 226560\n",
      "[1,960,7,7]\n",
      "2 Clip_93 [1, 960, 7, 7] 188160 188160\n",
      "[1,960,7,7]\n",
      "2 Conv_94 [1, 960, 7, 7] 188160 188160\n",
      "3 Conv_94 [320, 960, 1, 1] 1228800 1416960\n",
      "3 Conv_94 [320] 1280 1418240\n",
      "[1,320,7,7]\n",
      "2 Conv_95 [1, 320, 7, 7] 62720 62720\n",
      "3 Conv_95 [1280, 320, 1, 1] 1638400 1701120\n",
      "3 Conv_95 [1280] 5120 1706240\n",
      "[1,1280,7,7]\n",
      "2 Clip_96 [1, 1280, 7, 7] 250880 250880\n",
      "[1,1280,7,7]\n",
      "2 GlobalAveragePool_97 [1, 1280, 7, 7] 250880 250880\n",
      "[1,1280,7,7]\n",
      "2 Shape_98 [1, 1280, 7, 7] 250880 250880\n",
      "[4]\n",
      "2 Gather_100 [4] 32 32\n",
      "[]\n",
      "2 Gather_100 [] 8 40\n",
      "[]\n",
      "2 Unsqueeze_101 [] 8 8\n",
      "[1]\n",
      "2 Concat_102 [1] 8 8\n",
      "3 Concat_102 [1] 8 16\n",
      "[1,1280,1,1]\n",
      "2 Reshape_103 [1, 1280, 1, 1] 5120 5120\n",
      "[2]\n",
      "2 Reshape_103 [2] 16 5136\n",
      "[]\n",
      "2 Gemm_104 [] 4 4\n",
      "3 Gemm_104 [1000, 1280] 5120000 5120004\n",
      "3 Gemm_104 [1000] 4000 5124004\n",
      "layer                   each_layer\n",
      "--------------------  ------------\n",
      "Gemm_104                   5124004\n",
      "Conv_7                     4820736\n",
      "Clip_6                     4816896\n",
      "Conv_14                    1820256\n",
      "Conv_12                    1812096\n",
      "Conv_18                    1812096\n",
      "Clip_11                    1806336\n",
      "Clip_13                    1806336\n",
      "Clip_17                    1806336\n",
      "Conv_95                    1706240\n",
      "Conv_4                     1607744\n",
      "Conv_2                     1606912\n",
      "Clip_1                     1605632\n",
      "Clip_3                     1605632\n",
      "Conv_94                    1418240\n",
      "Conv_9                     1213536\n",
      "Clip_8                     1204224\n",
      "Conv_5                      809344\n",
      "Conv_82                     803200\n",
      "Conv_88                     803200\n",
      "Conv_65                     673152\n",
      "Conv_71                     673152\n",
      "Conv_78                     649600\n",
      "Conv_84                     649600\n",
      "Conv_90                     649600\n",
      "Conv_25                     626816\n",
      "Conv_31                     626816\n",
      "Conv_23                     609792\n",
      "Conv_29                     609792\n",
      "Conv_35                     609792\n",
      "Conv_0                      605696\n",
      "Add_15                      602112\n",
      "Clip_22                     602112\n",
      "Clip_24                     602112\n",
      "Clip_28                     602112\n",
      "Clip_30                     602112\n",
      "Clip_34                     602112\n",
      "Conv_77                     482176\n",
      "Conv_63                     474624\n",
      "Conv_69                     474624\n",
      "Conv_75                     474624\n",
      "Conv_20                     470144\n",
      "Clip_19                     451584\n",
      "Clip_62                     451584\n",
      "Clip_64                     451584\n",
      "Clip_68                     451584\n",
      "Clip_70                     451584\n",
      "Clip_74                     451584\n",
      "Conv_60                     448896\n",
      "Conv_42                     399616\n",
      "Conv_48                     399616\n",
      "Conv_54                     399616\n",
      "Conv_40                     316416\n",
      "Conv_46                     316416\n",
      "Conv_52                     316416\n",
      "Conv_58                     316416\n",
      "Conv_10                     315456\n",
      "Conv_16                     315456\n",
      "Clip_39                     301056\n",
      "Clip_41                     301056\n",
      "Clip_45                     301056\n",
      "Clip_47                     301056\n",
      "Clip_51                     301056\n",
      "Clip_53                     301056\n",
      "Clip_57                     301056\n",
      "Clip_59                     301056\n",
      "Conv_61                     298752\n",
      "Conv_67                     298752\n",
      "Conv_73                     298752\n",
      "Clip_96                     250880\n",
      "GlobalAveragePool_97        250880\n",
      "Shape_98                    250880\n",
      "Conv_80                     226560\n",
      "Conv_86                     226560\n",
      "Conv_92                     226560\n",
      "Add_26                      200704\n",
      "Add_32                      200704\n",
      "Conv_37                     199936\n",
      "Clip_79                     188160\n",
      "Clip_81                     188160\n",
      "Clip_85                     188160\n",
      "Clip_87                     188160\n",
      "Clip_91                     188160\n",
      "Clip_93                     188160\n",
      "Clip_36                     150528\n",
      "Add_66                      150528\n",
      "Add_72                      150528\n",
      "Conv_38                     150016\n",
      "Conv_44                     150016\n",
      "Conv_50                     150016\n",
      "Conv_56                     150016\n",
      "Conv_21                     125696\n",
      "Conv_27                     125696\n",
      "Conv_33                     125696\n",
      "Clip_76                     112896\n",
      "Add_43                      100352\n",
      "Add_49                      100352\n",
      "Add_55                      100352\n",
      "Add_83                       62720\n",
      "Add_89                       62720\n",
      "Reshape_103                   5136\n",
      "Gather_100                      40\n",
      "Concat_102                      16\n",
      "Unsqueeze_101                    8\n",
      "Constant_99                      0\n",
      "====================================================================================\n",
      "\n",
      "The activation memory for the model is: \n",
      " 67676148 (bytes)\n",
      " 67.676148 (MB)\n",
      "\n",
      "op_name    unfound_tensor    op_type\n",
      "---------  ----------------  ---------\n",
      "====================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import shape_inference\n",
    "from os import path\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from onnx import onnx_ml_pb2 as xpb2\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(\"./mobilenetv2-10.onnx\", load_external_data=False)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "inferred_model = shape_inference.infer_shapes(onnx_model)\n",
    "print('shape inference complete ...')\n",
    "\n",
    "def _parse_element(elem: xpb2.ValueInfoProto):\n",
    "    name = getattr(elem, 'name', \"None\")\n",
    "    data_type = \"NA\"\n",
    "    shape_str = \"NA\"\n",
    "    etype = getattr(elem, 'type', False)\n",
    "    # print(elem.name)\n",
    "    if etype:\n",
    "        ttype = getattr(etype, 'tensor_type', False)\n",
    "        if ttype:\n",
    "            data_type = getattr(ttype, 'elem_type', 0)\n",
    "            shape = getattr(elem.type.tensor_type, \"shape\", False)\n",
    "            # print(name,shape)\n",
    "            if shape:\n",
    "                shape_str = \"[\"\n",
    "                dims = getattr(shape, 'dim', [])\n",
    "                # print(dims)\n",
    "                for dim in dims:\n",
    "                    vals = getattr(dim,'dim_param',\"?\")\n",
    "                    if vals: vals=1\n",
    "                    else:vals = getattr(dim, 'dim_value', \"?\")\n",
    "                    # print(vals)\n",
    "                    if vals == 0: vals=1\n",
    "                    shape_str += (str(vals) + \",\")\n",
    "                shape_str = shape_str.rstrip(\",\")\n",
    "                shape_str += \"]\"\n",
    "                print(shape_str)\n",
    "    return name, data_type, shape_str\n",
    "\n",
    "def get_valueproto_or_tensorproto_by_name(name: str, graph: xpb2.GraphProto):\n",
    "    for i, node in enumerate(inferred_model.graph.node):\n",
    "            if node.name == \"\":\n",
    "                inferred_model.graph.node[i].name = str(i)\n",
    "    input_nlist = [k.name for k in graph.input]\n",
    "    initializer_nlist = [k.name for k in graph.initializer]\n",
    "    value_info_nlist = [k.name for k in graph.value_info]\n",
    "    output_nlist = [k.name for k in graph.output]\n",
    "\n",
    "    # get tensor data\n",
    "    if name in input_nlist:\n",
    "        idx = input_nlist.index(name)\n",
    "        return graph.input[idx], int(1)\n",
    "    elif name in value_info_nlist:\n",
    "        idx = value_info_nlist.index(name)\n",
    "        return graph.value_info[idx], int(2)\n",
    "    elif name in initializer_nlist:\n",
    "        idx = initializer_nlist.index(name)\n",
    "        return graph.initializer[idx], int(3)\n",
    "    elif name in output_nlist:\n",
    "        idx = output_nlist.index(name)\n",
    "        return graph.output[idx], int(4)\n",
    "    else:\n",
    "        print(\"[ERROR MASSAGE] Can't find the tensor: \", name)\n",
    "        print('input_nlist:\\n', input_nlist)\n",
    "        print('===================')\n",
    "        print('value_info_nlist:\\n', value_info_nlist)\n",
    "        print('===================')\n",
    "        print('initializer_nlist:\\n', initializer_nlist)\n",
    "        print('===================')\n",
    "        print('output_nlist:\\n', output_nlist)\n",
    "        print('===================')\n",
    "        return False, 0\n",
    "\n",
    "def cal_tensor_mem_size(elem_type: str, shape: [int]):\n",
    "    \"\"\" given the element type of the tensor and its shape, and return its memory size.\n",
    "\n",
    "    Utility.\n",
    "\n",
    "    Args:\n",
    "        ttype: the type of the element of the given tensor. format: 'int', ...\n",
    "        shape: the shape of the given tensor. format: [] of int\n",
    "\n",
    "    Returns:\n",
    "        mem_size: int\n",
    "    \"\"\"\n",
    "    # init\n",
    "    mem_size = int(1)\n",
    "    # traverse the list to get the number of the elements\n",
    "    for num in shape:\n",
    "        mem_size *= num\n",
    "    # multiple the size of variable with the number of the elements\n",
    "    # \"FLOAT\": 1,\n",
    "    # \"UINT8\": 2,\n",
    "    # \"INT8\": 3,\n",
    "    # \"UINT16\": 4,\n",
    "    # \"INT16\": 5,\n",
    "    # \"INT32\": 6,\n",
    "    # \"INT64\": 7,\n",
    "    # # \"STRING\" : 8,\n",
    "    # \"BOOL\": 9,\n",
    "    # \"FLOAT16\": 10,\n",
    "    # \"DOUBLE\": 11,\n",
    "    # \"UINT32\": 12,\n",
    "    # \"UINT64\": 13,\n",
    "    # \"COMPLEX64\": 14,\n",
    "    # \"COMPLEX128\": 15\n",
    "    if elem_type == 1:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 2:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 3:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 4:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 5:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 6:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 7:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 9:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 10:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 11:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 12:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 13:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 14:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 15:\n",
    "        mem_size *= 16\n",
    "    else:\n",
    "        print(\"Undefined data type\")\n",
    "\n",
    "    return mem_size\n",
    "\n",
    "\n",
    "\n",
    "def get_bandwidth(graph: xpb2.GraphProto):\n",
    "    try:\n",
    "        Activation_list = []\n",
    "        total_activation = 0\n",
    "        unknown_tensor_list = []\n",
    "        # traverse all the nodes\n",
    "        for nodeProto in graph.node:\n",
    "            # init variables\n",
    "            # print(nodeProto)\n",
    "            read_mem_BW_each_layer = 0\n",
    "            write_mem_BW_each_layer = 0\n",
    "            total_each_layer = 0\n",
    "            # traverse all input tensor\n",
    "            for input_name in nodeProto.input:\n",
    "                # get the TensorProto/ValueInfoProto by searching its name\n",
    "                proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                    input_name, graph)\n",
    "                # if nodeProto.op_type == \"Gemm\": print(type_Num)\n",
    "                # print(proto,type_Num)\n",
    "                # parse the ValueInfoProto/TensorProto\n",
    "                if proto:\n",
    "                    if type_Num == 3:\n",
    "                        dtype = getattr(proto, 'data_type', False)\n",
    "                        # get the shape of the tensor\n",
    "                        shape = getattr(proto, 'dims', [])\n",
    "                        # print(shape_str)\n",
    "                        # print(proto.name,shape)\n",
    "                        \n",
    "                    elif type_Num == 1 or type_Num == 2:\n",
    "                        name, dtype, shape_str = _parse_element(proto)\n",
    "                        # print(proto.name,shape_str)\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        # print(name,shape_str)\n",
    "                        shape = []\n",
    "                        # print(f\"3 = {dtype}, shape={shape}\")\n",
    "                        for dim in shape_str:\n",
    "                            if len(dim) > 0:\n",
    "                                shape.append(int(dim))\n",
    "                    else:\n",
    "                        print(\n",
    "                            '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                            input_name, ' is from a wrong list !')\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is no found !')\n",
    "                    unknown_tensor_list.append(\n",
    "                        (nodeProto.name, input_name, nodeProto.op_type))\n",
    "                # calculate the tensor size in btye\n",
    "                # print(nodeProto)\n",
    "                # if(len(shape)<3) and type_Num==3 :continue\n",
    "                read_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "                print(type_Num,nodeProto.name,shape,cal_tensor_mem_size(dtype, shape),read_mem_BW_each_layer)\n",
    "            \n",
    "            total_each_layer = read_mem_BW_each_layer\n",
    "\n",
    "            # store into tuple\n",
    "            temp_tuple = (nodeProto.name, total_each_layer)\n",
    "            #append it\n",
    "            Activation_list.append(temp_tuple)\n",
    "            # accmulate the value\n",
    "            total_activation += total_each_layer\n",
    "\n",
    "        # display the mem_bw of eahc layer\n",
    "        columns = ['layer','each_layer']\n",
    "        # resort the list\n",
    "        Activation_list = sorted(Activation_list,\n",
    "                             key=lambda Layer: Layer[1],\n",
    "                             reverse=True)\n",
    "        print(tabulate(Activation_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "        # display it\n",
    "        print(\n",
    "            \"The activation memory for the model is: \\n\",\n",
    "            total_activation, '(bytes)\\n',\n",
    "            float(total_activation) / float(1000000), '(MB)\\n')\n",
    "        # display the unknown tensor\n",
    "        columns = ['op_name', 'unfound_tensor', 'op_type']\n",
    "        print(tabulate(unknown_tensor_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR MASSAGE] Unable to display: \" + str(e))\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "#從這裡開始\n",
    "print(\"start\")\n",
    "get_bandwidth(inferred_model.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca4df57b-3b5c-4f5a-959d-8358308759a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inference complete ...\n",
      "list_of_layer RESORTED ! \n",
      "Conv_name               Output_tensor\n",
      "--------------------  ---------------\n",
      "Conv_5                        4816896\n",
      "Clip_6                        4816896\n",
      "Conv_10                       1806336\n",
      "Conv_12                       1806336\n",
      "Conv_16                       1806336\n",
      "Clip_11                       1806336\n",
      "Clip_13                       1806336\n",
      "Clip_17                       1806336\n",
      "Conv_0                        1605632\n",
      "Conv_2                        1605632\n",
      "Clip_1                        1605632\n",
      "Clip_3                        1605632\n",
      "Conv_7                        1204224\n",
      "Clip_8                        1204224\n",
      "Conv_4                         802816\n",
      "Conv_21                        602112\n",
      "Conv_23                        602112\n",
      "Conv_27                        602112\n",
      "Conv_29                        602112\n",
      "Conv_33                        602112\n",
      "Clip_22                        602112\n",
      "Clip_24                        602112\n",
      "Clip_28                        602112\n",
      "Clip_30                        602112\n",
      "Clip_34                        602112\n",
      "Conv_18                        451584\n",
      "Conv_61                        451584\n",
      "Conv_63                        451584\n",
      "Conv_67                        451584\n",
      "Conv_69                        451584\n",
      "Conv_73                        451584\n",
      "Clip_19                        451584\n",
      "Clip_62                        451584\n",
      "Clip_64                        451584\n",
      "Clip_68                        451584\n",
      "Clip_70                        451584\n",
      "Clip_74                        451584\n",
      "Conv_9                         301056\n",
      "Conv_14                        301056\n",
      "Conv_38                        301056\n",
      "Conv_40                        301056\n",
      "Conv_44                        301056\n",
      "Conv_46                        301056\n",
      "Conv_50                        301056\n",
      "Conv_52                        301056\n",
      "Conv_56                        301056\n",
      "Conv_58                        301056\n",
      "Clip_39                        301056\n",
      "Clip_41                        301056\n",
      "Clip_45                        301056\n",
      "Clip_47                        301056\n",
      "Clip_51                        301056\n",
      "Clip_53                        301056\n",
      "Clip_57                        301056\n",
      "Clip_59                        301056\n",
      "Add_15                         301056\n",
      "Conv_95                        250880\n",
      "Clip_96                        250880\n",
      "Conv_78                        188160\n",
      "Conv_80                        188160\n",
      "Conv_84                        188160\n",
      "Conv_86                        188160\n",
      "Conv_90                        188160\n",
      "Conv_92                        188160\n",
      "Clip_79                        188160\n",
      "Clip_81                        188160\n",
      "Clip_85                        188160\n",
      "Clip_87                        188160\n",
      "Clip_91                        188160\n",
      "Clip_93                        188160\n",
      "Conv_35                        150528\n",
      "Clip_36                        150528\n",
      "Conv_75                        112896\n",
      "Clip_76                        112896\n",
      "Conv_20                        100352\n",
      "Conv_25                        100352\n",
      "Conv_31                        100352\n",
      "Add_26                         100352\n",
      "Add_32                         100352\n",
      "Conv_60                         75264\n",
      "Conv_65                         75264\n",
      "Conv_71                         75264\n",
      "Add_66                          75264\n",
      "Add_72                          75264\n",
      "Conv_94                         62720\n",
      "Conv_37                         50176\n",
      "Conv_42                         50176\n",
      "Conv_48                         50176\n",
      "Conv_54                         50176\n",
      "Add_43                          50176\n",
      "Add_49                          50176\n",
      "Add_55                          50176\n",
      "Conv_77                         31360\n",
      "Conv_82                         31360\n",
      "Conv_88                         31360\n",
      "Add_83                          31360\n",
      "Add_89                          31360\n",
      "GlobalAveragePool_97             5120\n",
      "Gemm_104                         4000\n",
      "Shape_98                           32\n",
      "Concat_102                         16\n",
      "Constant_99                         8\n",
      "Gather_100                          8\n",
      "Unsqueeze_101                       8\n",
      "Reshape_103                         4\n",
      "Total memory size = 52010348\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import shape_inference\n",
    "from os import path\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from onnx import onnx_ml_pb2 as xpb2\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(\"./mobilenetv2-10.onnx\", load_external_data=False)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "inferred_model = shape_inference.infer_shapes(onnx_model)\n",
    "print('shape inference complete ...')\n",
    "\n",
    "def _parse_element(elem: xpb2.ValueInfoProto):\n",
    "    name = getattr(elem, 'name', \"None\")\n",
    "    data_type = \"NA\"\n",
    "    shape_str = \"NA\"\n",
    "    etype = getattr(elem, 'type', False)\n",
    "    # print(elem.name)\n",
    "    if etype:\n",
    "        ttype = getattr(etype, 'tensor_type', False)\n",
    "        if ttype:\n",
    "            data_type = getattr(ttype, 'elem_type', 0)\n",
    "            shape = getattr(elem.type.tensor_type, \"shape\", False)\n",
    "            # print(name,shape)\n",
    "            if shape:\n",
    "                shape_str = \"[\"\n",
    "                dims = getattr(shape, 'dim', [])\n",
    "                # print(dims)\n",
    "                for dim in dims:\n",
    "                    vals = getattr(dim,'dim_param',\"?\")\n",
    "                    if vals: vals=1\n",
    "                    else:vals = getattr(dim, 'dim_value', \"?\")\n",
    "                    # print(vals)\n",
    "                    if vals == 0: vals=1\n",
    "                    shape_str += (str(vals) + \",\")\n",
    "                shape_str = shape_str.rstrip(\",\")\n",
    "                shape_str += \"]\"\n",
    "                # print(shape_str)\n",
    "    return name, data_type, shape_str\n",
    "\n",
    "def get_valueproto_or_tensorproto_by_name(name: str, graph: xpb2.GraphProto):\n",
    "    for i, node in enumerate(inferred_model.graph.node):\n",
    "            if node.name == \"\":\n",
    "                inferred_model.graph.node[i].name = str(i)\n",
    "    input_nlist = [k.name for k in graph.input]\n",
    "    initializer_nlist = [k.name for k in graph.initializer]\n",
    "    value_info_nlist = [k.name for k in graph.value_info]\n",
    "    output_nlist = [k.name for k in graph.output]\n",
    "\n",
    "    # get tensor data\n",
    "    if name in input_nlist:\n",
    "        idx = input_nlist.index(name)\n",
    "        return graph.input[idx], int(1)\n",
    "    elif name in value_info_nlist:\n",
    "        idx = value_info_nlist.index(name)\n",
    "        return graph.value_info[idx], int(2)\n",
    "    elif name in initializer_nlist:\n",
    "        idx = initializer_nlist.index(name)\n",
    "        return graph.initializer[idx], int(3)\n",
    "    elif name in output_nlist:\n",
    "        idx = output_nlist.index(name)\n",
    "        return graph.output[idx], int(4)\n",
    "    else:\n",
    "        print(\"[ERROR MASSAGE] Can't find the tensor: \", name)\n",
    "        print('input_nlist:\\n', input_nlist)\n",
    "        print('===================')\n",
    "        print('value_info_nlist:\\n', value_info_nlist)\n",
    "        print('===================')\n",
    "        print('initializer_nlist:\\n', initializer_nlist)\n",
    "        print('===================')\n",
    "        print('output_nlist:\\n', output_nlist)\n",
    "        print('===================')\n",
    "        return False, 0\n",
    "\n",
    "def cal_tensor_mem_size(elem_type: str, shape: [int]):\n",
    "    \"\"\" given the element type of the tensor and its shape, and return its memory size.\n",
    "\n",
    "    Utility.\n",
    "\n",
    "    Args:\n",
    "        ttype: the type of the element of the given tensor. format: 'int', ...\n",
    "        shape: the shape of the given tensor. format: [] of int\n",
    "\n",
    "    Returns:\n",
    "        mem_size: int\n",
    "    \"\"\"\n",
    "    # init\n",
    "    mem_size = int(1)\n",
    "    # traverse the list to get the number of the elements\n",
    "    for num in shape:\n",
    "        mem_size *= num\n",
    "    # multiple the size of variable with the number of the elements\n",
    "    # \"FLOAT\": 1,\n",
    "    # \"UINT8\": 2,\n",
    "    # \"INT8\": 3,\n",
    "    # \"UINT16\": 4,\n",
    "    # \"INT16\": 5,\n",
    "    # \"INT32\": 6,\n",
    "    # \"INT64\": 7,\n",
    "    # # \"STRING\" : 8,\n",
    "    # \"BOOL\": 9,\n",
    "    # \"FLOAT16\": 10,\n",
    "    # \"DOUBLE\": 11,\n",
    "    # \"UINT32\": 12,\n",
    "    # \"UINT64\": 13,\n",
    "    # \"COMPLEX64\": 14,\n",
    "    # \"COMPLEX128\": 15\n",
    "    if elem_type == 1:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 2:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 3:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 4:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 5:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 6:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 7:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 9:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 10:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 11:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 12:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 13:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 14:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 15:\n",
    "        mem_size *= 16\n",
    "    else:\n",
    "        print(\"Undefined data type\")\n",
    "\n",
    "    return mem_size\n",
    "    \n",
    "def get_activation_memory(graph: xpb2.GraphProto):\n",
    "\n",
    "    All_tensor_size = []\n",
    "    Total_size =0 \n",
    "    \n",
    "    node_nList = [k.name for k in graph.node]\n",
    "    input_nlist = [k.name for k in graph.input]\n",
    "    initializer_nlist = [k.name for k in graph.initializer]\n",
    "    value_info_nlist = [k.name for k in graph.value_info]\n",
    "    output_nlist = [k.name for k in graph.output]\n",
    "    idx_list = {}\n",
    "    for node in graph.node:\n",
    "        if node.op_type in idx_list:\n",
    "            idx_list[node.op_type][node.name] = node_nList.index(node.name)\n",
    "        else:\n",
    "            idx_list[node.op_type] = {\n",
    "                node.name: node_nList.index(node.name)\n",
    "            }\n",
    "    All_type = idx_list.keys()\n",
    "    for ty in All_type:\n",
    "        for idx in idx_list[ty].values():\n",
    "            # print(ty,idx)\n",
    "            list_of_data_num = []\n",
    "\n",
    "            # get input tensor proto\n",
    "            # for input_name in graph.node[idx].input:\n",
    "            #     # print(input_name)\n",
    "                \n",
    "            #     # get tensor data\n",
    "            #     if input_name in input_nlist:\n",
    "            #         name_idx = input_nlist.index(input_name)\n",
    "            #         data = graph.input[name_idx]\n",
    "            #         type_num = int(1)\n",
    "            #     elif input_name in value_info_nlist:\n",
    "            #         name_idx = value_info_nlist.index(input_name)\n",
    "            #         data = graph.value_info[name_idx]\n",
    "            #         type_num = int(2)\n",
    "            #     elif input_name in initializer_nlist:\n",
    "            #         name_idx = initializer_nlist.index(input_name)\n",
    "            #         data = graph.initializer[name_idx]\n",
    "            #         type_num = int(3)\n",
    "            #     elif input_name in output_nlist:\n",
    "            #         name_idx = output_nlist.index(input_name)\n",
    "            #         data = graph.output[name_idx]\n",
    "            #         type_num = int(4)\n",
    "            #     else:\n",
    "            #         print(\"Can't find the tensor: \", input_name)\n",
    "            #         print('input_nlist:\\n', input_nlist)\n",
    "            #         print('===================')\n",
    "            #         print('value_info_nlist:\\n', value_info_nlist)\n",
    "            #         print('===================')\n",
    "            #         print('initializer_nlist:\\n', initializer_nlist)\n",
    "            #         print('===================')\n",
    "            #         print('output_nlist:\\n', output_nlist)\n",
    "            #         print('===================')\n",
    "            \n",
    "            #     list_of_data_num.append((data, type_num))\n",
    "                # print(len(list_of_data_num))\n",
    "            list_temp = [\n",
    "                    graph.node[idx].name,\n",
    "                ]\n",
    "            if graph.node[idx].output[0] in input_nlist:\n",
    "                name_idx = input_nlist.index(graph.node[idx].output[0])\n",
    "                data = graph.input[name_idx]\n",
    "                type_num = int(1)\n",
    "            elif graph.node[idx].output[0] in value_info_nlist:\n",
    "                name_idx = value_info_nlist.index(graph.node[idx].output[0])\n",
    "                data = graph.value_info[name_idx]\n",
    "                type_num = int(2)\n",
    "            elif graph.node[idx].output[0] in initializer_nlist:\n",
    "                name_idx = initializer_nlist.index(graph.node[idx].output[0])\n",
    "                data = graph.initializer[name_idx]\n",
    "                type_num = int(3)\n",
    "            elif graph.node[idx].output[0] in output_nlist:\n",
    "                name_idx = output_nlist.index(graph.node[idx].output[0])\n",
    "                data = graph.output[name_idx]\n",
    "                type_num = int(4)\n",
    "            else:\n",
    "                print(\"Can't find the tensor: \", graph.node[idx].output[0])\n",
    "                print('input_nlist:\\n', input_nlist)\n",
    "                print('===================')\n",
    "                print('value_info_nlist:\\n', value_info_nlist)\n",
    "                print('===================')\n",
    "                print('initializer_nlist:\\n', initializer_nlist)\n",
    "                print('===================')\n",
    "                print('output_nlist:\\n', output_nlist)\n",
    "                print('===================')\n",
    "            # list_of_data_num.append((data, type_num))\n",
    "            # shape = getattr(proto, 'dims', [])\n",
    "            # print(graph.node[idx].name,_parse_element(data))\n",
    "            name, data_type, shape_str = _parse_element(data)\n",
    "            shape_str = shape_str.strip('[]')\n",
    "            shape_str = shape_str.split(',')\n",
    "            shape = []\n",
    "            for dim in shape_str:\n",
    "                if not dim: continue\n",
    "                else:shape.append(int(dim))\n",
    "            mem_size = cal_tensor_mem_size(data_type, shape)\n",
    "            Total_size += mem_size\n",
    "            list_temp.append(mem_size)\n",
    "            # print(shape,list_temp)\n",
    "            All_tensor_size.append(list_temp)\n",
    "    columns = [\n",
    "            'Conv_name', \n",
    "            'Output_tensor'\n",
    "        ]\n",
    "\n",
    "    # resort the list\n",
    "    All_tensor_size = sorted(All_tensor_size,\n",
    "                                  key=lambda Layer: Layer[-1],\n",
    "                                  reverse=True)\n",
    "    print('list_of_layer RESORTED ! ')\n",
    "    print(tabulate(All_tensor_size, headers=columns))\n",
    "    print(f\"Total memory size = {Total_size}\")\n",
    "\n",
    "        \n",
    "# name, dtype, shape_str = _parse_element(proto)\n",
    "# dtype = getattr(proto, 'data_type', False)\n",
    "get_activation_memory(inferred_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feaf2c5-1f43-4ae4-81a8-681bc58048a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "import onnx\n",
    "from onnx import helper, numpy_helper\n",
    "\n",
    "# 创建 Gemm 节点\n",
    "gemm_node = helper.make_node(\n",
    "    'Gemm',  # 节点类型\n",
    "    ['A', 'B', 'C'],  # 输入\n",
    "    ['Y'],  # 输出\n",
    "    alpha=1.0,  # alpha 参数\n",
    "    beta=1.0,  # beta 参数\n",
    "    transA=0,  # transA 参数\n",
    "    transB=0   # transB 参数\n",
    ")\n",
    "\n",
    "# 创建图\n",
    "graph = helper.make_graph(\n",
    "    [gemm_node],\n",
    "    \"gemm_example\",\n",
    "    [\n",
    "        helper.make_tensor_value_info('A', onnx.TensorProto.FLOAT, [3, 4]),\n",
    "        helper.make_tensor_value_info('B', onnx.TensorProto.FLOAT, [4, 5]),\n",
    "        helper.make_tensor_value_info('C', onnx.TensorProto.FLOAT, [3, 5])\n",
    "    ],\n",
    "    [helper.make_tensor_value_info('Y', onnx.TensorProto.FLOAT, [3, 5])]\n",
    ")\n",
    "\n",
    "# 创建模型\n",
    "model = helper.make_model(graph)\n",
    "\n",
    "# 保存模型\n",
    "onnx.save(model, \"gemm_example.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
